---
title: "Survival Analysis - Binary Classification Model"
author: "R. Jerome Dixon"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
format:
  html:
    toc: true
    toc-depth: 5
    code-fold: true
    code-summary: "Show the code"
    embed-resources: true
    default-image-extension: svg
    dpi: 600
---

```{r load-libraries}
#| echo: true
#| warning: false
#| message: false
#| results: hide

library(here)
library(dplyr)
library(readr)
library(magrittr)
library(spatstat)
library(tibble)
library(ggplot2)
library(purrr)
library(tidyverse)
library(huxtable)
library(reticulate)
library(DT)
library(caret)
library(forcats)
library(jsonlite)
library(quarto)

(options)(scipen=999)

```

### Survival Model Dataset

```{r load-dataset}
#| echo: true
#| warning: false
#| message: false
#| eval: true

set.seed(1997)

survival_model <- read_rds(here("data","survival_analysis.rds"))

survival_model %<>%
  mutate_if(is.character, as.factor) 
 
survival_model %>% 
  str()


```

####  Format Dataset 

```{r format-dataset}

#| echo: true
#| warning: false
#| message: false
#| eval: true

survival_model %<>%
  mutate(INIT_STAT = as.factor(INIT_STAT)) %>%
  mutate(INOTROPES_TCR = as.factor(INOTROPES_TCR)) %>%
  mutate(PGE_TCR = as.factor(PGE_TCR)) %>%
  mutate_if(is.factor, ~fct_na_value_to_level(., "Unknown")) %>% 
  select(-c( days_total, WL_ID_CODE, PT_CODE)) %>% 
  select(-c(BMI, PGE_TCR, WL_OTHER_ORG, LISTING_CTR_CODE )) %>% 
  filter(!is.na(outcome_final))

survival_model %>% 
  str()

```

- Target Label is 'outcome_final'

```{r}

survival_model %<>% 
  rename(
    `Blood Type` = ABO,
    `Gender` = GENDER,
    `Race` = RACE,
    `Diabetes` = DIAB,
    `Height` = HEIGHT_CM,
    `Weight` = WEIGHT_KG,
    `ECMO at Reg` = ECMO_CAND_REG,
    `VAD Device TCR` = VAD_DEVICE_TY_TCR,
    `Ventilator at Reg` = VENTILATOR_CAND_REG,
    `Defribilator` = IMPL_DEFIBRIL,
    `Recent Creatinine` = MOST_RCNT_CREAT,
    `Inotropes TCR` = INOTROPES_TCR,
    `Functional Status at Registration` = FUNC_STAT_CAND_REG,
    `Total Albumin Level` = TOT_SERUM_ALBUM,
    `Candidate Diagnosis` = CAND_DIAG,
    `LC Average Volume of Txp/Year` = LISTING_CTR_TX_AVG,
    `Initial Status` = INIT_STAT,
    `Age` = AGE,
    #`LC Code` = LISTING_CTR_CODE,
    `LC Median # Offer Refusals` = median_refusals,
    `LC Median Wait Days` = median_wait_days,
    `LC Median Wait Days at Initial Status` = median_wait_days_status
    
  )

```

```{r}

cat_features <- names(survival_model)[sapply(survival_model, is.factor)]

cat_features

```

### CatBoost Encoding

```{r} 

#| echo: true
#| warning: false
#| message: false
#| eval: true

features_cb <- survival_model %>% 
  select(-outcome_final)


# Must be factor or numeric "Class"
model_target <- survival_model %>% 
  select(outcome_final) %>% 
  dummify() %>% 
  as.data.frame()

target_cb <- model_target$outcome_final

```

#### Utility Function for Categorical Indexes needed by CatBoost

```{python}

def get_categorical_indexes(X_train):
    # Select columns with object or categorical dtype
    categorical_columns = X_train.select_dtypes(include=['object', 'category'])

    # Get the column indexes of categorical variables
    categorical_indexes = [X_train.columns.get_loc(col) for col in categorical_columns]

    return categorical_indexes

```


```{python catboost-r-data}

#| echo: true
#| warning: false
#| message: false
#| results: hide

import numpy as np
from catboost import Pool

# initialize data
X = r.features_cb

y = r.target_cb

cat_index = get_categorical_indexes(X)

# Using this object for feature importance calculations
test_data = catboost_pool = Pool(X, y, cat_index)


```


#### Model

- From Optuna

```{python optuna-params1}

model_params = {
    'learning_rate': 0.25,
    'depth': 1,
    'colsample_bylevel': 0.25,
    'min_data_in_leaf': 36,
    'l2_leaf_reg': 8.45
}
    
```

```{python catboost-model-logloss}

#| eval: true
#| echo: true
#| message: false
#| warning: false
#| results: hide

import numpy as np
from sklearn.model_selection import train_test_split, StratifiedKFold
import pandas as pd
import optuna
from catboost import CatBoostClassifier, Pool
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score

# Convert target label from list to array
Y = np.array(y)  

X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, train_size=0.80, stratify=y, random_state=1997)

cat_indexes = get_categorical_indexes(X_train)

model_logloss = CatBoostClassifier(iterations=1000,
                               objective='Logloss',
                               **model_params, 
                               boosting_type= 'Ordered',
                               bootstrap_type='MVS',
                               metric_period=25,
                               early_stopping_rounds=100,
                               use_best_model=True, random_seed=1997)
                               

# Create a Pool object for the training and testing data
train_pool = Pool(X_train, cat_features=cat_indexes, label=Y_train)
test_pool = Pool(X_validation, cat_features=cat_indexes, label=Y_validation)
 

model_logloss.fit(train_pool, eval_set=test_pool)

```

#### Model Accuracy

```{python}

import pandas as pd

Y_Pred = model_logloss.predict(X_validation)
Y_Pred_Proba = model_logloss.predict_proba(X_validation)[:, 1]  # get the probabilities of the positive class


Y_Pred_Proba_Positive = model_logloss.predict_proba(X_validation)[:, 1]  # Probabilities of the positive class
Y_Pred_Proba_Negative = model_logloss.predict_proba(X_validation)[:, 0]  # Probabilities of the negative class

# Converting predictions and actuals into a DataFrame for better readability, including negative class probabilities
predictions = pd.DataFrame({
    'Prob_Negative_Class': Y_Pred_Proba_Negative,
    'Prob_Positive_Class': Y_Pred_Proba_Positive,
    'Predicted': Y_Pred,
    'Actual': Y_validation
})

# Converting predictions and actuals into a DataFrame for better readability
predictions = pd.DataFrame({
    'Prob_Positive_Class': Y_Pred_Proba,
    'Predicted': Y_Pred,
    'Actual': Y_validation
})

```

##### Uncalibrated Predictions

```{r message=FALSE, warning=FALSE}

library(probably)

predictions <- py$predictions %>% 
  mutate(Class = ifelse(Actual == 0, "survive", "not_survive"),
         .pred_not_survive = Prob_Positive_Class
         )

# Define the levels you want
factor_levels <- c("survive", "not_survive")

# Set the levels of the 'actuals' column
predictions$Class <- factor(predictions$Class, levels = rev(factor_levels))

predictions %>% 
  datatable()

```

##### Calibration Plot - CatBoost

```{r}

predictions %>% 
  cal_plot_logistic(Class, .pred_not_survive)

```

##### Decision Threshold

```{r message=FALSE, warning=FALSE}

library(pROC)

# Calculate the ROC curve
roc_result <- roc(predictions$Actual, predictions$Prob_Positive_Class)

coords <- coords(roc_result, "best", ret="threshold", best.method="closest.topleft")

# Optimal threshold for maximizing true positive rate
optimal_threshold <- coords$threshold

# Apply the optimal threshold to convert probabilities to class predictions
predictions$predicted_classes <- ifelse(predictions$Prob_Positive_Class >= optimal_threshold, 1, 0)

# Output the optimal threshold
cat("Optimal Threshold:", optimal_threshold, "\n")

cat("Number of '1's predicted:", sum(predictions$predicted_classes), "\n")

```

##### Calibrated Model Accuracy Using Decision Threshold

```{python}

#| echo: true
#| warning: false
#| message: false
#| eval: false

from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, roc_curve, roc_auc_score

# Calculate metrics
f1_updated = f1_score(r.predictions["Actual"], r.predictions["predicted_classes"])
precision_updated = precision_score(r.predictions["Actual"], r.predictions["predicted_classes"])
recall_updated = recall_score(r.predictions["Actual"], r.predictions["predicted_classes"])
accuracy_updated = accuracy_score(r.predictions["Actual"], r.predictions["predicted_classes"])

# Calculate the area under the curve (AUC)
model_auc_updated = roc_auc_score(r.predictions["Actual"], r.predictions["Prob_Positive_Class"])
conf_matrix_updated = confusion_matrix(r.predictions["Actual"], r.predictions["predicted_classes"])

```

```{r updated-metrics-catboost-native}

#| echo: true
#| warning: false
#| message: false
#| eval: false

model_f1_score <- py$f1_updated
model_recall <- py$recall_updated
model_precision <- py$precision_updated
model_accuracy <- py$accuracy_updated
model_roc_score <- py$model_auc_updated

model_accuracy_updated <- list(
  F1_Score = model_f1_score,
  Recall = model_recall,
  Precision = model_precision,
  Accuracy = model_accuracy,
  ROC_Score = model_roc_score
)

model_accuracy_updated

```

```{python feature-importance}
#| echo: true
#| warning: false
#| message: false
#| results: hide
#| eval: true

gain = model_logloss.get_feature_importance(prettified=True)
loss = model_logloss.get_feature_importance(test_data, type='LossFunctionChange', prettified=True)

```

### One Hot Encoding

```{r}
survival_model_one_hot <- survival_model
```

```{r}

# Create the dummy variables specification
dummies <- dummyVars(~ ., data = survival_model_one_hot[, cat_features], fullRank = FALSE)

# Generate the dummy variables
df_dummies <- predict(dummies, newdata = survival_model_one_hot)

# Bind the new dummy variables with the original dataframe minus the original factor columns
survival_model_one_hot <- cbind(survival_model_one_hot[, !(names(survival_model_one_hot) %in% cat_features)], df_dummies)

# Review the structure of the updated dataframe
str(survival_model_one_hot)


```

```{r}

features_cb <- survival_model_one_hot %>% 
  select(-outcome_final)

# Must be factor or numeric "Class"
model_target <- survival_model_one_hot %>% 
  select(outcome_final) %>% 
  dummify() %>% 
  as.data.frame()

target_cb <- model_target$outcome_final

```


```{python catboost-r-data2}

#| echo: true
#| warning: false
#| message: false
#| results: hide

import numpy as np
from catboost import Pool

# initialize data
X = r.features_cb

y = r.target_cb

# Using this object for feature importance calculations
test_data = catboost_pool = Pool(X, y)

```

- From Optuna

```{python optuna-params2}

model_params2 = {
    'learning_rate': 0.25,
    'depth': 3,
    'colsample_bylevel': 0.05,
    'min_data_in_leaf': 30,
    'l2_leaf_reg': 10.51
}
    
```

```{python catboost-model-logloss-one-hot-encoding}

#| eval: true
#| echo: true
#| message: false
#| warning: false
#| results: hide

from sklearn.model_selection import train_test_split, StratifiedKFold
from catboost import CatBoostClassifier, Pool


X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, train_size=0.80, stratify=y)

survival_model = CatBoostClassifier(iterations=1000,
                               objective='Logloss',
                               **model_params2, 
                               boosting_type= 'Ordered',
                               metric_period=25,
                               bootstrap_type='MVS',
                               early_stopping_rounds=100,
                               use_best_model=True, random_seed=1997)
                               

# Create a Pool object for the training and testing data
train_pool = Pool(X_train, label=Y_train)
test_pool = Pool(X_validation, label=Y_validation)
 

survival_model.fit(train_pool, eval_set=test_pool)

gain = survival_model.get_feature_importance(prettified=True)
loss = survival_model.get_feature_importance(test_data, type='LossFunctionChange', prettified=True)

```

#### Model Accuracy

```{python}

import pandas as pd

Y_Pred = survival_model.predict(X_validation)
Y_Pred_Proba = survival_model.predict_proba(X_validation)[:, 1]  # get the probabilities of the positive class


Y_Pred_Proba_Positive = survival_model.predict_proba(X_validation)[:, 1]  # Probabilities of the positive class
Y_Pred_Proba_Negative = survival_model.predict_proba(X_validation)[:, 0]  # Probabilities of the negative class

# Converting predictions and actuals into a DataFrame for better readability, including negative class probabilities
predictions = pd.DataFrame({
    'Prob_Negative_Class': Y_Pred_Proba_Negative,
    'Prob_Positive_Class': Y_Pred_Proba_Positive,
    'Predicted': Y_Pred,
    'Actual': Y_validation
})

# Converting predictions and actuals into a DataFrame for better readability
predictions = pd.DataFrame({
    'Prob_Positive_Class': Y_Pred_Proba,
    'Predicted': Y_Pred,
    'Actual': Y_validation
})

```

##### Uncalibrated Predictions

```{r message=FALSE, warning=FALSE}

library(probably)

predictions <- py$predictions %>% 
  mutate(Class = ifelse(Actual == 0, "survive", "not_survive"),
         .pred_not_survive = Prob_Positive_Class
         )

# Define the levels you want
factor_levels <- c("survive", "not_survive")

# Set the levels of the 'actuals' column
predictions$Class <- factor(predictions$Class, levels = rev(factor_levels))

predictions %>% 
  datatable()

```

##### Calibration Plot - CatBoost

```{r}

predictions %>% 
  cal_plot_logistic(Class, .pred_not_survive)

```

##### Decision Threshold

```{r message=FALSE, warning=FALSE}

library(pROC)

# Calculate the ROC curve
roc_result <- roc(predictions$Actual, predictions$Prob_Positive_Class)

coords <- coords(roc_result, "best", ret="threshold", best.method="closest.topleft")

# Optimal threshold for maximizing true positive rate
optimal_threshold <- coords$threshold

# Apply the optimal threshold to convert probabilities to class predictions
predictions$predicted_classes <- ifelse(predictions$Prob_Positive_Class >= optimal_threshold, 1, 0)

# Output the optimal threshold
cat("Optimal Threshold:", optimal_threshold, "\n")

cat("Number of '1's predicted:", sum(predictions$predicted_classes), "\n")

```

##### Calibrated Threshold Model Accuracy Using LogLoss

```{python}

#| echo: true
#| warning: false
#| message: false
#| eval: false

from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, roc_curve, roc_auc_score

# Calculate metrics
f1_updated = f1_score(r.predictions["Actual"], r.predictions["predicted_classes"])
precision_updated = precision_score(r.predictions["Actual"], r.predictions["predicted_classes"])
recall_updated = recall_score(r.predictions["Actual"], r.predictions["predicted_classes"])
accuracy_updated = accuracy_score(r.predictions["Actual"], r.predictions["predicted_classes"])

# Calculate the area under the curve (AUC)
model_auc_updated = roc_auc_score(r.predictions["Actual"], r.predictions["Prob_Positive_Class"])
conf_matrix_updated = confusion_matrix(r.predictions["Actual"], r.predictions["predicted_classes"])

```

```{r updated-metrics-one-hot}

#| echo: true
#| warning: false
#| message: false
#| eval: false

model_f1_score <- py$f1_updated
model_recall <- py$recall_updated
model_precision <- py$precision_updated
model_accuracy <- py$accuracy_updated
model_roc_score <- py$model_auc_updated

model_accuracy_one_hot <- list(
  F1_Score = model_f1_score,
  Recall = model_recall,
  Precision = model_precision,
  Accuracy = model_accuracy,
  ROC_Score = model_roc_score
)

model_accuracy_one_hot

```

### Numerical Encoding

#### Feature Importance - From One Hot Encoding Model

```{r}

gain_tbl <- py$gain
loss_tbl <- py$loss

num_features <- nrow(loss_tbl) # Total number of features

importances <- loss_tbl %>%
  mutate(
    Scaled_Importances = scale(Importances)[, 1],
    Min_Importance = min(Scaled_Importances),
    Max_Importance = max(Scaled_Importances),
    Weighted_Score = ((Scaled_Importances - Min_Importance) / (Max_Importance - Min_Importance)) * num_features
  ) %>%
  select(-Scaled_Importances, -Min_Importance, -Max_Importance)


importances %>% 
  datatable()

```

```{r}

cat_encoded_features <- features_cb[10:75]

for (col in names(cat_encoded_features)) {
  if (any(cat_encoded_features[[col]] == 1, na.rm = TRUE)) {
    sum_col <- sum(cat_encoded_features[[col]], na.rm = TRUE)
    cat_encoded_features <- cat_encoded_features %>%
      mutate(!!col := if_else(!!sym(col) == 1, sum_col, !!sym(col)))
  }
}

# Check the result
cat_encoded_features %>% 
  head(15) %>% 
  datatable()

```


```{r}

#| eval: false
#| echo: true
#| warning: false
#| message: false
#| results: hide


# Scale frequency count by feature importances
for (i in 1:nrow(importances)) {
  feature <- importances$`Feature Id`[i]
  score <- importances$Weighted_Score[i]
  
  if (feature %in% names(cat_encoded_features)) {
    cat_encoded_features <- cat_encoded_features %>%
      mutate(!!feature := if_else(!!sym(feature) != 0, as.integer(!!sym(feature) * score), !!sym(feature)))

  }
}


cat_encoded_features %>% 
  head(15) %>% 
  datatable()

```

```{r}

# Distinct Numerical Encodings
numerical_encoding <- cat_encoded_features %>%
  pivot_longer(cols = everything(), names_to = "Feature", values_to = "Encoded_Value") %>% 
  filter(Encoded_Value > 0) %>% 
  unique() %>% 
  arrange(-Encoded_Value)

numerical_encoding %>% 
  datatable()

```


```{r}

# Split Encoded_Feature into Feature and Level using regex to handle backticks and dots
encodings <- numerical_encoding %>%
  mutate(
    Feature_Level = str_split_fixed(Feature, "(?<=`|\\.)", 3),
    Feature = str_remove_all(Feature_Level[, 1], "`|\\."),
    Level = str_remove_all(Feature_Level[, 2], "`|\\.") ,
    Feature = ifelse(Feature == "", str_remove_all(Feature_Level[, 2], "`|\\."), Feature),
    Level = ifelse(Feature_Level[,1] == "`", str_remove_all(Feature_Level[, 3], "`|\\."), Feature_Level[,2])
  ) %>% 
  select(Feature,Level,Encoded_Value) %>% 
  arrange(-Encoded_Value)


encodings %>% 
  datatable()


```


```{r}

# Initialize the updated dataframe
df_updated <- survival_model

# Replace categorical values with numerical encodings
for (feature in unique(encodings$Feature)) {
  encoding_map <- encodings %>%
    filter(Feature == feature) %>%
    select(Level, Encoded_Value) %>%
    deframe()
  
  df_updated[[feature]] <- as.character(df_updated[[feature]])
  df_updated[[feature]] <- as.numeric(encoding_map[df_updated[[feature]]])
}

# Print the updated dataframe to verify
df_updated %>% 
  head(15) %>% 
  datatable()

```


```{r}

features_cb <- df_updated %>% 
  select(-outcome_final)

# Must be factor or numeric "Class"
model_target <- df_updated %>% 
  select(outcome_final) %>% 
  dummify() %>% 
  as.data.frame()

target_cb <- model_target$outcome_final

```

```{python catboost-r-data3}

#| echo: true
#| warning: false
#| message: false
#| results: hide

import numpy as np
from catboost import Pool

# initialize data
X = r.features_cb

y = r.target_cb

# Using this object for feature importance calculations
test_data = catboost_pool = Pool(X, y)


```

- From Optuna

```{python optuna-params3}

model_params3 = {
    'learning_rate': 0.2,
    'depth': 3,
    'colsample_bylevel': 0.06,
    'min_data_in_leaf': 97,
    'l2_leaf_reg': 6.26
}
    
```

```{python catboost-model-logloss-numerical-encoding}

#| eval: true
#| echo: true
#| message: false
#| warning: false
#| results: hide

from sklearn.model_selection import train_test_split, StratifiedKFold
from catboost import CatBoostClassifier, Pool


X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, train_size=0.80, stratify=y)

model_numeric = CatBoostClassifier(iterations=1000,
                               objective='Logloss',
                               **model_params3, 
                               boosting_type= 'Ordered',
                               metric_period=25,
                               bootstrap_type='MVS',
                               early_stopping_rounds=100,
                               use_best_model=True, random_seed=1997)
                               

# Create a Pool object for the training and testing data
train_pool = Pool(X_train, label=Y_train)
test_pool = Pool(X_validation, label=Y_validation)
 

model_numeric.fit(train_pool, eval_set=test_pool)

```

#### Model Accuracy

```{python}

import pandas as pd

Y_Pred = model_numeric.predict(X_validation)
Y_Pred_Proba = model_numeric.predict_proba(X_validation)[:, 1]  # get the probabilities of the positive class


Y_Pred_Proba_Positive = model_numeric.predict_proba(X_validation)[:, 1]  # Probabilities of the positive class
Y_Pred_Proba_Negative = model_numeric.predict_proba(X_validation)[:, 0]  # Probabilities of the negative class

# Converting predictions and actuals into a DataFrame for better readability, including negative class probabilities
predictions = pd.DataFrame({
    'Prob_Negative_Class': Y_Pred_Proba_Negative,
    'Prob_Positive_Class': Y_Pred_Proba_Positive,
    'Predicted': Y_Pred,
    'Actual': Y_validation
})

# Converting predictions and actuals into a DataFrame for better readability
predictions = pd.DataFrame({
    'Prob_Positive_Class': Y_Pred_Proba,
    'Predicted': Y_Pred,
    'Actual': Y_validation
})

```

##### Uncalibrated Predictions

```{r message=FALSE, warning=FALSE}

library(probably)

predictions <- py$predictions %>% 
  mutate(Class = ifelse(Actual == 0, "survive", "not_survive"),
         .pred_not_survive = Prob_Positive_Class
         )

# Define the levels you want
factor_levels <- c("survive", "not_survive")

# Set the levels of the 'actuals' column
predictions$Class <- factor(predictions$Class, levels = rev(factor_levels))

predictions %>% 
  datatable()

```

##### Calibration Plot - CatBoost

```{r}

predictions %>% 
  cal_plot_logistic(Class, .pred_not_survive)

```

##### Decision Threshold

```{r message=FALSE, warning=FALSE}

library(pROC)

# Calculate the ROC curve
roc_result <- roc(predictions$Actual, predictions$Prob_Positive_Class)

coords <- coords(roc_result, "best", ret="threshold", best.method="closest.topleft")

# Optimal threshold for maximizing true positive rate
optimal_threshold <- coords$threshold

# Apply the optimal threshold to convert probabilities to class predictions
predictions$predicted_classes <- ifelse(predictions$Prob_Positive_Class >= optimal_threshold, 1, 0)

# Output the optimal threshold
cat("Optimal Threshold:", optimal_threshold, "\n")

cat("Number of '1's predicted:", sum(predictions$predicted_classes), "\n")

```

##### Calibrated Threshold Model Accuracy Using LogLoss

```{python}

#| echo: true
#| warning: false
#| message: false
#| eval: false

from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, roc_curve, roc_auc_score

# Calculate metrics
f1_updated = f1_score(r.predictions["Actual"], r.predictions["predicted_classes"])
precision_updated = precision_score(r.predictions["Actual"], r.predictions["predicted_classes"])
recall_updated = recall_score(r.predictions["Actual"], r.predictions["predicted_classes"])
accuracy_updated = accuracy_score(r.predictions["Actual"], r.predictions["predicted_classes"])

# Calculate the area under the curve (AUC)
model_auc_updated = roc_auc_score(r.predictions["Actual"], r.predictions["Prob_Positive_Class"])
conf_matrix_updated = confusion_matrix(r.predictions["Actual"], r.predictions["predicted_classes"])

```

```{r updated-metrics-custom-encoding}

#| echo: true
#| warning: false
#| message: false
#| eval: false

model_f1_score <- py$f1_updated
model_recall <- py$recall_updated
model_precision <- py$precision_updated
model_accuracy <- py$accuracy_updated
model_roc_score <- py$model_auc_updated

model_accuracy_numeric <- list(
  F1_Score = model_f1_score,
  Recall = model_recall,
  Precision = model_precision,
  Accuracy = model_accuracy,
  ROC_Score = model_roc_score
)

model_accuracy_numeric

```

### Regression Model

#### Top Features

```{python}

import pandas as pd
import json

sorted_loss = loss.sort_values(by='Importances', ascending=False)

# Get the top 15 rows with the highest importances
top_15_loss = sorted_loss.head(15)

# Combine the lists and remove duplicates
final_top_features = top_15_loss['Feature Id'].tolist()

# Helper function to chunk the list
def chunk_list(lst, n):
    """Yield successive n-sized chunks from lst."""
    for i in range(0, len(lst), n):
        yield lst[i:i + n]

# Chunk the list into sublists of 4 elements each
chunked_packages = list(chunk_list(final_top_features, 4))

# Convert each chunk into a JSON string and collect them into a list
json_chunks = [json.dumps(chunk) for chunk in chunked_packages]

# Join all chunks with a line break to ensure each chunk is on a new line
final_features_top = "\n".join(json_chunks)

print(final_features_top)

```

```{r}

final_features <- py$final_top_features

```

```{r}

set.seed(1997)

survival_model_one_hot %<>% 
  drop_na()

X <- survival_model_one_hot %>% 
  select(all_of(final_features))

y <- survival_model_one_hot$outcome_final

index <- createDataPartition(y, p = 0.80, list = FALSE, times = 1)
train <- X[index, ]
test <- X[-index, ]
y_train <- y[index]
y_test <- y[-index]


```


```{r regression, message=FALSE}

train_data <- data.frame(y=y_train, train)

glm_model <- glm(y ~ ., data = train_data, family = binomial)

# Summary of the logistic regression model
lr_summary <- summary(glm_model)

# Reorder columns in test_data to match train_data
test <- test[, names(train)]

test_data <- data.frame(y=y_test, test)

# Predict probabilities on test set
lr_proba <- predict(glm_model, newdata = test_data, type = "response")

# Convert probabilities to class predictions
lr_class <- ifelse(lr_proba > 0.5, 1, 0)
names(lr_class) <- "lr_class"

```

```{r message=FALSE}
table(lr_class)
```


### Lasso Penalized Logistic Regression

```{r message=FALSE}

# Create design matrices for both train and test datasets
train_matrix <- model.matrix(~ ., data = train)
test_matrix <- model.matrix(~ ., data = test)

```


```{r lasso, message=FALSE}

library(glmnet)

# Perform cross-validation to select lambda
cv_lasso <- cv.glmnet(train_matrix, y_train, family = "binomial", alpha = 1)

# Predict probabilities on test set
lasso_proba <- predict(cv_lasso, newx = test_matrix, 
                       type = "response", s = "lambda.min")
names(lasso_proba) <- "lasso_proba"

# Convert probabilities to class predictions
lasso_class <- ifelse(lasso_proba > 0.5, 1, 0)
names(lasso_class) <- "lasso_class"

```

```{r message=FALSE}
table(lasso_class)
```

### Lasso Penalized Logistic Regression - Relaxed Fit

```{r lasso-relaxed, message=FALSE}

# Perform cross-validation to select lambda
cv_lasso_relaxed <- cv.glmnet(train_matrix, y_train, family = "binomial", alpha = 1, relax = TRUE)

# Predict probabilities on test set
lasso_relaxed_proba <- predict(cv_lasso_relaxed, newx = test_matrix, 
                       type = "response", s = "lambda.min", gamma = "gamma.min")

names(lasso_relaxed_proba) <- "lasso_relaxed_proba"

# Convert probabilities to class predictions
lasso_relaxed_class <- ifelse(lasso_relaxed_proba > 0.5, 1, 0)
names(lasso_relaxed_class) <- "lasso_relaxed_class"

```

```{r message=FALSE}
table(lasso_relaxed_class)
```

