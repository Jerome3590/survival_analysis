---
title: "Survival Analysis: CatBoost vs XGBoost vs RandomForest"
author: "R. Jerome Dixon"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
format:
  html:
    toc: true
    toc-depth: 5
    code-fold: true
    code-summary: "Show the code"
    embed-resources: true
    default-image-extension: svg
    dpi: 600
---

```{r load-libraries}
#| echo: true
#| warning: false
#| message: false

library(here)
library(dplyr)
library(readr)
library(magrittr)
library(spatstat)
library(tibble)
library(ggplot2)
library(purrr)
library(tidyverse)
library(huxtable)
library(reticulate)
library(DT)
library(caret)
library(glmnet)
library(forcats)
library(jsonlite)
library(quarto)
library(plotly)

(options)(scipen=999)

```

### Survival Model Dataset

```{r load-dataset}
#| echo: true
#| warning: false
#| message: false
#| eval: true

set.seed(1997)

survival_model <- read_rds(here("data","model_data_train.rds"))

survival_model %<>%
  mutate_if(is.character, as.factor) 

colnames(survival_model)

```

```{r}

survival_model %<>% 
  select(-median_refusals_old,-LC_effect, -mean_refusals, -WL_ID_CODE, -WL_DT, -LIST_YR, -CITIZENSHIP, -starts_with("median_wait_days"), -starts_with("DONCRIT"), -HEMODYNAMICS_CO, -INOTROP_VASO_CO_REG)

```

```{r}
#| echo: false
#| warning: false
#| message: false
#| eval: true

survival_model_test <- read_rds(here("data","model_data_test.rds"))

survival_model_test %<>%
  mutate_if(is.character, as.factor) 


```

```{r}

survival_model_test %<>% 
  select(-median_refusals_old, -LC_effect, -mean_refusals, -WL_ID_CODE, -WL_DT, -LIST_YR, -CITIZENSHIP, -starts_with("median_wait_days"), -starts_with("DONCRIT"), -HEMODYNAMICS_CO, -INOTROP_VASO_CO_REG)

```

```{r}

cat_features_train <- names(survival_model)[sapply(survival_model, is.factor)]

cat_features_train

```

```{r}

cat_features_test <- names(survival_model_test)[sapply(survival_model_test, is.factor)]

cat_features_test

```

#### Utility Function for Categorical Indexes

```{python}
#| echo: false
#| warning: false
#| message: false
#| eval: true

def get_categorical_indexes(X_train):
    # Select columns with object or categorical dtype
    categorical_columns = X_train.select_dtypes(include=['object', 'category'])

    # Get the column indexes of categorical variables
    categorical_indexes = [X_train.columns.get_loc(col) for col in categorical_columns]

    return categorical_indexes

```

### Native CatBoost Model

```{r}
#| echo: false
#| warning: false
#| message: false
#| eval: true

# Train
train_data <- survival_model %>% 
  select(-outcome)

# Must be factor or numeric "Class"
train_target <- survival_model %>% 
  select(outcome) %>% 
  dummify() %>% 
  as.data.frame()

train_Y <- survival_model$outcome


# Test
test_data <- survival_model_test %>% 
  select(-outcome)

# Must be factor or numeric "Class"
test_target <- survival_model_test %>% 
  select(outcome) %>% 
  dummify() %>% 
  as.data.frame()

test_Y <- survival_model_test$outcome

```

```{python catboost-r-data}

#| echo: false
#| warning: false
#| message: false

import numpy as np

# initialize Train and Test datasets
X_train = r.train_data
y_train = r.train_Y
Y_train = np.array(y_train)  

X_test = r.test_data
y_test = r.test_Y
Y_test = np.array(y_test) 

cat_index = get_categorical_indexes(X_train)

```

```{python}
#| echo: false
#| warning: false
#| message: false

import pandas as pd

# Convert NaN values to a string in categorical columns
categorical_columns = [col for col in X_train.columns if X_train[col].dtype == 'object']
X_train[categorical_columns] = X_train[categorical_columns].fillna('missing')


# Convert NaN values to a string in categorical columns
categorical_columns = [col for col in X_test.columns if X_test[col].dtype == 'object']
X_test[categorical_columns] = X_test[categorical_columns].fillna('missing')

```

-   Data Cleansing for Missing Values

```{python}
#| echo: false
#| warning: false
#| message: false

# NaN values in the column at index 15
has_nan = X_train.iloc[:, 14].isna().any()
print("Column at index 14 contains NaN values:", has_nan)

```

```{python}
#| echo: false
#| warning: false
#| message: false

if 'Missing' not in X_train.iloc[:, 14].cat.categories:
    X_train.iloc[:, 14] = X_train.iloc[:, 14].cat.add_categories(['Missing'])

# Replace NaN values in the column at index 15 with "Missing"
X_train.iloc[:, 14] = X_train.iloc[:, 14].fillna('Missing')

```

#### Optuna Hyperparameter Optimization

```{python optuna-native-model, eval=FALSE}

#| eval: false
#| echo: false
#| warning: false
#| message: false

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
import catboost as cb
from catboost import Pool
from catboost.utils import eval_metric
import optuna


def objective(trial):
    # Parameter suggestions
    params = {
        "objective": "Logloss",
        "eval_metric":"AUC",
        "iterations": 1000,
        "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3),
        "depth": trial.suggest_int("depth", 1, 9),
        "colsample_bylevel": trial.suggest_float("colsample_bylevel", 0.05, 1.0),
        "min_data_in_leaf": trial.suggest_int("min_data_in_leaf", 1, 100),
        "l2_leaf_reg": trial.suggest_float("l2_leaf_reg", 1, 12),
        "boosting_type": "Ordered",
        "bootstrap_type": "MVS",
        "verbose": 0  # Controlling verbose output
    }

    model = cb.CatBoostClassifier(**params)
    train_pool = cb.Pool(X_train, Y_train, cat_features=get_categorical_indexes(X_train))
    cv_results = cb.cv(train_pool, params, fold_count=3, seed=3590, stratified=True, verbose=False, plot=False)
    return np.max(cv_results['test-AUC-mean'])

study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=25, timeout=6000)

print("Number of finished trials: {}".format(len(study.trials)))
print("Best trial:")
for key, value in study.best_trial.params.items():
    print("  {}: {}".format(key, value))


```
- Best is trial #18/25 with value: 0.74067

```{python optuna-params1}

model_params1 = {
    'learning_rate': 0.238,
    'depth': 2,
    'colsample_bylevel': 0.326,
    'min_data_in_leaf': 6,
    'l2_leaf_reg': 8.41
}
    
```

```{python catboost-model-auc}

#| eval: true
#| echo: true
#| message: false
#| warning: false

import numpy as np
from sklearn.model_selection import train_test_split, StratifiedKFold
import pandas as pd
import optuna
from catboost import CatBoostClassifier, Pool
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score


cat_indexes = get_categorical_indexes(X_train)

model_auc = CatBoostClassifier(iterations=1000,
                               objective='Logloss',
                               eval_metric="AUC",
                               **model_params1, 
                               boosting_type= 'Ordered',
                               bootstrap_type='MVS',
                               metric_period=25,
                               early_stopping_rounds=100,
                               use_best_model=True, 
                               random_seed=1997)
                               

# Create a Pool object for the training and testing data
train_pool = Pool(X_train, cat_features=cat_indexes, label=Y_train)
test_pool = Pool(X_test, cat_features=cat_indexes, label=Y_test)
 

model_auc.fit(train_pool, eval_set=test_pool)

```

#### Model Metrics

```{python}

from sklearn.metrics import roc_auc_score, brier_score_loss, accuracy_score, log_loss
import pandas as pd

Y_Pred = model_auc.predict(X_test)

Y_Pred_Proba_Positive = model_auc.predict_proba(X_test)[:, 1]  # Probabilities of the positive class

# Calculate AUC
auc = roc_auc_score(Y_test, Y_Pred_Proba_Positive)
# Calculate Brier Score
brier_score = brier_score_loss(Y_test, Y_Pred_Proba_Positive)
# Calculate Accuracy
accuracy = accuracy_score(Y_test, Y_Pred)
# Calculate Log Loss
log_loss_value = log_loss(Y_test, Y_Pred_Proba_Positive)

# Create a DataFrame
catboost_native = pd.DataFrame({
    'Model': ['Native_Catboost'],
    'AUC': [auc],
    'Brier Score': [brier_score],
    'Accuracy': [accuracy],
    'Log Loss': [log_loss_value]
})

print(catboost_native)

```

#### Feature Importances: Native Model

```{python}

gain = model_auc.get_feature_importance(prettified=True)
loss = model_auc.get_feature_importance(test_pool, type='LossFunctionChange', prettified=True)

```

```{r catboost-native-feature-importance}
#| echo: false
#| layout-ncol: 1
#| label: tbl-feature-importance-native
#| tbl-cap: "CatBoost Feature Importance"
#| tbl-subcap: 
#|   - "Gain"
#|   - "Loss Function Change"
#| warning: false
#| message: false
#| eval: true

gain_tbl1 <- py$gain

gain_table1 <- tibble( 'Feature ID' = gain_tbl1$`Feature Id`,
                  'Importance' = gain_tbl1$Importances) %>% 
  rowid_to_column(var = "Rank") %>% 
  as_hux() %>%
  theme_article() %>% 
  set_align(col=c('Rank','Importance'), value= "center") %>% 
  set_tb_padding(2)

loss_tbl1 <- py$loss

loss_table1 <- tibble( 'Feature ID' = loss_tbl1$`Feature Id`,
                  'Importance' = loss_tbl1$Importances) %>% 
  rowid_to_column(var = "Rank") %>% 
  as_hux() %>%
  theme_article() %>% 
  set_align(col=c('Rank','Importance'), value= "center") %>% 
  set_tb_padding(2)


gain_table1
loss_table1

```

### CatBoost Filtered by Feature Importances

Here we filter for features both common in Gain Feature Importances and Loss Value Function Change Feature Importances.

```{python}

import pandas as pd

# Filter the features with importance greater than zero
gain_filtered = gain[gain['Importances'] > 0]
loss_filtered = loss[loss['Importances'] > 0]

# Find the intersection of features
common_features = set(gain_filtered['Feature Id']).intersection(set(loss_filtered['Feature Id']))

# Display the common features
common_features_df = pd.DataFrame({'Feature': list(common_features)})

common_features_df

```

```{python}
#| echo: false
#| warning: false
#| message: false

import numpy as np

common_features = common_features_df['Feature'].tolist()

# Filter the train and test datasets to include only the common features
X_train_filtered = X_train[common_features]
X_test_filtered = X_test[common_features]

# Initialize Train and Test datasets
y_train = r.train_Y
Y_train = np.array(y_train)  

y_test = r.test_Y
Y_test = np.array(y_test)  

cat_index = get_categorical_indexes(X_train_filtered)

```

#### Optuna Hyperparameter Optimization

```{python optuna-feature-importance-model, eval=FALSE}

#| eval: false
#| echo: false
#| warning: false
#| message: false

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
import catboost as cb
from catboost import Pool
from catboost.utils import eval_metric
import optuna


def objective(trial):
    # Parameter suggestions
    params = {
        "objective": "Logloss",
        "eval_metric":"AUC",
        "iterations": 1000,
        "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3),
        "depth": trial.suggest_int("depth", 1, 9),
        "colsample_bylevel": trial.suggest_float("colsample_bylevel", 0.05, 1.0),
        "min_data_in_leaf": trial.suggest_int("min_data_in_leaf", 1, 100),
        "l2_leaf_reg": trial.suggest_float("l2_leaf_reg", 1, 12),
        "boosting_type": "Ordered",
        "bootstrap_type": "MVS",
        "verbose": 0  # Controlling verbose output
    }

    model = cb.CatBoostClassifier(**params)
    train_pool = cb.Pool(X_train_filtered, Y_train, cat_features=get_categorical_indexes(X_train_filtered))
    cv_results = cb.cv(train_pool, params, fold_count=3, seed=3590, stratified=True, verbose=False, plot=False)
    return np.max(cv_results['test-AUC-mean'])

study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=25, timeout=6000)

print("Number of finished trials: {}".format(len(study.trials)))
print("Best trial:")
for key, value in study.best_trial.params.items():
    print("  {}: {}".format(key, value))


```

```{python optuna-params2}

model_params2 = {
    'learning_rate': 0.12,
    'depth': 5,
    'colsample_bylevel': 0.0639,
    'min_data_in_leaf': 21,
    'l2_leaf_reg': 5.88
}
    
```

```{python catboost-filtered-by-feature-importance}

#| eval: true
#| echo: true
#| message: false
#| warning: false

import numpy as np
from sklearn.model_selection import train_test_split, StratifiedKFold
import pandas as pd
import optuna
from catboost import CatBoostClassifier, Pool
from sklearn.metrics import roc_auc_score, brier_score_loss, accuracy_score, log_loss, confusion_matrix


cat_indexes = get_categorical_indexes(X_train_filtered)

feature_importance_model = CatBoostClassifier(iterations=1000,
                               objective='Logloss',
                               eval_metric="AUC",
                               **model_params2, 
                               boosting_type= 'Ordered',
                               bootstrap_type='MVS',
                               metric_period=25,
                               early_stopping_rounds=100,
                               use_best_model=True, 
                               random_seed=1997)
                               

# Create a Pool object for the training and testing data
train_pool_filtered = Pool(X_train_filtered, cat_features=cat_indexes, label=Y_train)

test_pool_filtered = Pool(X_test_filtered, cat_features=cat_indexes, label=Y_test)
 

feature_importance_model.fit(train_pool_filtered, eval_set=test_pool_filtered)

```

```{python}

feature_importance_model.save_model("models/feature_importance",
           format="cbm",
           export_parameters=None,
           pool=None)


```

#### Model Metrics

```{python}
#| echo: false
#| warning: false
#| message: false

from sklearn.metrics import roc_auc_score, brier_score_loss, accuracy_score, log_loss
import pandas as pd

Y_Pred_filtered = feature_importance_model.predict(X_test_filtered)

Y_Pred_Proba_Positive_filtered = feature_importance_model.predict_proba(X_test_filtered)[:, 1]  # Probabilities of the positive class

# Calculate AUC
auc = roc_auc_score(Y_test, Y_Pred_Proba_Positive_filtered)
# Calculate Brier Score
brier_score = brier_score_loss(Y_test, Y_Pred_Proba_Positive_filtered)
# Calculate Accuracy
accuracy = accuracy_score(Y_test, Y_Pred_filtered)
# Calculate Log Loss
log_loss_value = log_loss(Y_test, Y_Pred_Proba_Positive_filtered)

# Create a DataFrame
feature_importance_model_df = pd.DataFrame({
    'Model': ['CB_FeatureImportance'],
    'AUC': [auc],
    'Brier Score': [brier_score],
    'Accuracy': [accuracy],
    'Log Loss': [log_loss_value]
})

print(feature_importance_model_df)


```

#### Feature Importances Updated

```{python}
#| echo: false
#| warning: false
#| message: false

gain2 = feature_importance_model.get_feature_importance(prettified=True)
loss2 = feature_importance_model.get_feature_importance(test_pool, type='LossFunctionChange', prettified=True)

```

```{r catboost-gain-feature-importance}
#| echo: true
#| layout-ncol: 1
#| label: tbl-feature-importance-feature-importance
#| tbl-cap: "CatBoost Feature Importance"
#| tbl-subcap: 
#|   - "Gain"
#|   - "Loss Function Change"
#| warning: false
#| message: false
#| eval: true

gain_tbl2 <- py$gain2

gain_table2 <- tibble( 'Feature ID' = gain_tbl2$`Feature Id`,
                  'Importance' = gain_tbl2$Importances) %>% 
  rowid_to_column(var = "Rank") %>% 
  as_hux() %>%
  theme_article() %>% 
  set_align(col=c('Rank','Importance'), value= "center") %>% 
  set_tb_padding(2)


loss_tbl2 <- py$loss2

loss_table2 <- tibble( 'Feature ID' = loss_tbl2$`Feature Id`,
                  'Importance' = loss_tbl2$Importances) %>% 
  rowid_to_column(var = "Rank") %>% 
  as_hux() %>%
  theme_article() %>% 
  set_align(col=c('Rank','Importance'), value= "center") %>% 
  set_tb_padding(2)

gain_table2
loss_table2

```

### CatBoost with One Hot Encode: Candidate Diagnosis

```{r}
#| echo: false
#| warning: false
#| message: false

# converting python objects to R objects to use existing one hot encoding workflow
X_train_filtered_ohe <- py$X_train_filtered
X_test_filtered_ohe <- py$X_test_filtered

# Initialize Train and Test datasets
y_train_ohe <- py$y_train
y_test_ohe <- py$y_test


cat_features_ohe <- names(X_train_filtered_ohe)[sapply(X_train_filtered_ohe, is.factor)]

cat_features_ohe


```

```{r}
#| echo: false
#| warning: false
#| message: false

# Create the dummy variables specification
dummies <- dummyVars(~ CAND_DIAG, data = X_train_filtered_ohe[, cat_features_ohe], fullRank = FALSE)

# Generate the dummy variables
df_dummies <- predict(dummies, newdata = X_train_filtered_ohe)

# Bind the new dummy variables with the original dataframe minus the original factor columns
X_train_filtered_ohe <- cbind(X_train_filtered_ohe[, !(names(X_train_filtered_ohe) %in% c('CAND_DIAG'))], df_dummies)

# Review the structure of the updated dataframe
colnames(X_train_filtered_ohe)

```

```{r}
#| echo: false
#| warning: false
#| message: false

# Create the dummy variables specification
dummies <- dummyVars(~ CAND_DIAG, data = X_test_filtered_ohe[, cat_features_ohe], fullRank = FALSE)

# Generate the dummy variables
df_dummies <- predict(dummies, newdata = X_test_filtered_ohe)

# Bind the new dummy variables with the original dataframe minus the original factor columns
X_test_filtered_ohe <- cbind(X_test_filtered_ohe[, !(names(X_test_filtered_ohe) %in% c('CAND_DIAG'))], df_dummies)

# Review the structure of the updated dataframe
colnames(X_test_filtered_ohe)

```

```{r}
#| echo: false
#| warning: false
#| message: false

X_train_filtered_ohe %<>% 
  rename(
    `CHD with Surgery` = `CAND_DIAG.Congenital Heart Disease With Surgery`,
    `CHD without Surgery` = `CAND_DIAG.Congenital Heart Disease Without Surgery`,
    `Dilated Cardiomyopathy` = `CAND_DIAG.Dilated Cardiomyopathy`,
    `Hypertrophic Cardiomyopathy` = `CAND_DIAG.Hypertrophic Cardiomyopathy`,
    `Myocarditis` = `CAND_DIAG.Myocarditis`,
    `CD_Other` = `CAND_DIAG.Other`,
    `Restrictive Cardiomyopathy` = `CAND_DIAG.Restrictive Cardiomyopathy`,
    `Valvular Heart Disease` = `CAND_DIAG.Valvular Heart Disease`
    
  )


X_test_filtered_ohe %<>% 
  rename(
    `CHD with Surgery` = `CAND_DIAG.Congenital Heart Disease With Surgery`,
    `CHD without Surgery` = `CAND_DIAG.Congenital Heart Disease Without Surgery`,
    `Dilated Cardiomyopathy` = `CAND_DIAG.Dilated Cardiomyopathy`,
    `Hypertrophic Cardiomyopathy` = `CAND_DIAG.Hypertrophic Cardiomyopathy`,
    `Myocarditis` = `CAND_DIAG.Myocarditis`,
    `CD_Other` = `CAND_DIAG.Other`,
    `Restrictive Cardiomyopathy` = `CAND_DIAG.Restrictive Cardiomyopathy`,
    `Valvular Heart Disease` = `CAND_DIAG.Valvular Heart Disease`
    
  )

```

```{python catboost-r-data2}
#| echo: false
#| warning: false
#| message: false

import numpy as np
from catboost import Pool

# initialize data
X_train_one_hot = r.X_train_filtered_ohe
Y = np.array(r.y_train_ohe)


X_test_one_hot = r.X_test_filtered_ohe
Y_test = np.array(r.y_test_ohe)

cat_index_one_hot = get_categorical_indexes(X_train_one_hot)


```

#### Optuna Hyperparameter Optimization

```{python optuna-one-hot-CD, eval=FALSE}

#| eval: false
#| echo: false
#| warning: false
#| message: false

import numpy as np
from sklearn.model_selection import train_test_split
import pandas as pd
import catboost as cb
from catboost.utils import eval_metric
import optuna


def objective(trial):
    # Parameter suggestions
    params = {
        "objective": "Logloss",
        "eval_metric":"AUC",
        "iterations": 1000,
        "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3),
        "depth": trial.suggest_int("depth", 1, 9),
        "colsample_bylevel": trial.suggest_float("colsample_bylevel", 0.05, 1.0),
        "min_data_in_leaf": trial.suggest_int("min_data_in_leaf", 1, 100),
        "l2_leaf_reg": trial.suggest_float("l2_leaf_reg", 1, 12),
        "boosting_type": "Ordered",
        "bootstrap_type": "MVS",
        "early_stopping_rounds": 100
    }

    model = cb.CatBoostClassifier(**params)
    train_pool = cb.Pool(X_train_one_hot, Y, cat_features=get_categorical_indexes(X_train_one_hot))
    cv_results = cb.cv(train_pool, params, fold_count=3, seed=3590, stratified=True, verbose=False)
    return np.max(cv_results['test-AUC-mean'])

study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=25, timeout=6000)

print("Number of finished trials: {}".format(len(study.trials)))
print("Best trial:")
for key, value in study.best_trial.params.items():
    print("  {}: {}".format(key, value))


```

-   From Optuna Trial #7/25 with value: 0.742

```{python optuna-params3}

model_params3 = {
    'learning_rate': 0.092,
    'depth': 7,
    'colsample_bylevel': 0.533,
    'min_data_in_leaf': 25,
    'l2_leaf_reg': 4.976
}
    
```

```{python catboost-model-one-hot-encoding}

#| eval: true
#| echo: true
#| message: false
#| warning: false

from sklearn.model_selection import train_test_split
from catboost import CatBoostClassifier, Pool


one_hot_model_CD = CatBoostClassifier(iterations=8000,
                               objective='Logloss',
                               eval_metric='AUC',
                               **model_params3, 
                               boosting_type= 'Ordered',
                               metric_period=500,
                               bootstrap_type='MVS',
                               early_stopping_rounds=100,
                               use_best_model=True, 
                               random_seed=1997)
                               

# Create a Pool object for the training and testing data
train_pool = Pool(X_train_one_hot, Y, cat_features=get_categorical_indexes(X_train_one_hot))
test_pool = Pool(X_test_one_hot, Y_test, cat_features=get_categorical_indexes(X_train_one_hot))
 

one_hot_model_CD.fit(train_pool, eval_set=test_pool)

gain_one_hot = one_hot_model_CD.get_feature_importance(prettified=True)
loss_one_hot = one_hot_model_CD.get_feature_importance(test_pool, type='LossFunctionChange', prettified=True)

```

```{python}
#| echo: false
#| warning: false
#| message: false

one_hot_model_CD.save_model("models/one_hot_CD",
           format="cbm",
           export_parameters=None,
           pool=None)


```

#### Feature Importances: Candidate Diagnosis One Hot Encoding

```{r feature-importance-one-hot}
#| echo: false
#| layout-ncol: 1
#| label: tbl-feature-importance-one-hot
#| tbl-cap: "CatBoost Feature Importance with One-Hot Encoding"
#| tbl-subcap: 
#|   - "Gain"
#|   - "Loss Function Change"
#| warning: false
#| message: false
#| eval: true

gain_tbl_ohe <- py$gain_one_hot
loss_tbl_ohe <- py$loss_one_hot

gain_table_ohe <- tibble( 'Feature ID' = gain_tbl_ohe$`Feature Id`,
                  'Importance' = gain_tbl_ohe$Importances) %>% 
  rowid_to_column(var = "Rank") %>% 
  as_hux() %>%
  theme_article() %>% 
  set_align(col=c('Rank','Importance'), value= "center") %>% 
  set_tb_padding(2)


loss_table_ohe <- tibble( 'Feature ID' = loss_tbl_ohe$`Feature Id`,
                  'Importance' = loss_tbl_ohe$Importances) %>% 
  rowid_to_column(var = "Rank") %>% 
  as_hux() %>%
  theme_article() %>% 
  set_align(col=c('Rank','Importance'), value= "center") %>% 
  set_tb_padding(2)

gain_table_ohe
loss_table_ohe

```

#### Model Metrics

```{python}
#| echo: false
#| warning: false
#| message: false

from sklearn.metrics import roc_auc_score, brier_score_loss, accuracy_score, log_loss
import pandas as pd

Y_Pred = one_hot_model_CD.predict(X_test_one_hot)

Y_Pred_Proba_Positive = one_hot_model_CD.predict_proba(X_test_one_hot)[:, 1]  # Probabilities of the positive class

# Calculate AUC
auc = roc_auc_score(Y_test, Y_Pred_Proba_Positive)
# Calculate Brier Score
brier_score = brier_score_loss(Y_test, Y_Pred_Proba_Positive)
# Calculate Accuracy
accuracy = accuracy_score(Y_test, Y_Pred)
# Calculate Log Loss
log_loss_value = log_loss(Y_test, Y_Pred_Proba_Positive)

# Create a DataFrame
candidate_diagnosis_one_hot = pd.DataFrame({
    'Model': ['CandidateDiagnosisOneHot'],
    'AUC': [auc],
    'Brier Score': [brier_score],
    'Accuracy': [accuracy],
    'Log Loss': [log_loss_value]
})

print(candidate_diagnosis_one_hot)


```

### CatBoost One Hot Encoding: All

```{r}
#| echo: false
#| warning: false
#| message: false

cat_features_ohe <- names(X_train_filtered_ohe)[sapply(X_train_filtered_ohe, is.factor)]

cat_features_ohe

```

```{r}
#| echo: false
#| warning: false
#| message: false

# Create the dummy variables specification for all categorical features
dummies <- dummyVars(~ ., data = X_train_filtered_ohe[, cat_features_ohe], fullRank = FALSE)

# Generate the dummy variables
df_dummies <- predict(dummies, newdata = X_train_filtered_ohe[, cat_features_ohe])

# Bind the new dummy variables with the original dataframe minus the original categorical columns
X_train_filtered_ohe <- cbind(X_train_filtered_ohe[, !(names(X_train_filtered_ohe) %in% cat_features_ohe)], df_dummies)

# Review the structure of the updated dataframe
colnames(X_train_filtered_ohe)

```

```{r}
#| echo: false
#| warning: false
#| message: false

# Create the dummy variables specification for all categorical features
dummies <- dummyVars(~ ., data = X_test_filtered_ohe[, cat_features_ohe], fullRank = FALSE)

# Generate the dummy variables
df_dummies <- predict(dummies, newdata = X_test_filtered_ohe[, cat_features_ohe])

# Bind the new dummy variables with the original dataframe minus the original categorical columns
X_test_filtered_ohe <- cbind(X_test_filtered_ohe[, !(names(X_test_filtered_ohe) %in% cat_features_ohe)], df_dummies)

# Review the structure of the updated dataframe
colnames(X_test_filtered_ohe)

```

```{r}
#| echo: true
#| warning: false
#| message: false

# Rename columns in the training dataset
X_train_filtered_ohe %<>% 
  rename(
    #CDL_CHD_With_Surgery = `CAND_DIAG_LISTING.Congenital Heart Disease With Surgery`,
    #CDL_CHD_Without_Surgery = `CAND_DIAG_LISTING.Congenital Heart Disease Without Surgery`,
    #CDL_Dilated_Cardiomyopathy = `CAND_DIAG_LISTING.Dilated Cardiomyopathy`,
    #CDL_Hypertrophic_Cardiomyopathy = `CAND_DIAG_LISTING.Hypertrophic Cardiomyopathy`,
    #CDL_Myocarditis = `CAND_DIAG_LISTING.Myocarditis`,
    #CDL_Other = `CAND_DIAG_LISTING.Other`,
    #CDL_Restrictive_Cardiomyopathy = `CAND_DIAG_LISTING.Restrictive Cardiomyopathy`,
    #CDL_Valvular_Heart_Disease = `CAND_DIAG_LISTING.Valvular Heart Disease`,
    CDC_1000 = `CAND_DIAG_CODE.1000: Dilated Cardiomyopathy`,
    CDC_1001 = `CAND_DIAG_CODE.1001: Dilated Cardiomyopathy`,
    CDC_1002 = `CAND_DIAG_CODE.1002: Dilated Cardiomyopathy`,
    CDC_1003 = `CAND_DIAG_CODE.1003: Dilated Cardiomyopathy`,
    CDC_1004 = `CAND_DIAG_CODE.1004: Myocarditis`,
    CDC_1005 = `CAND_DIAG_CODE.1005: Dilated Cardiomyopathy`,
    CDC_1006 = `CAND_DIAG_CODE.1006: Myocarditis`,
    CDC_1007 = `CAND_DIAG_CODE.1007: Dilated Cardiomyopathy`,
    CDC_1008 = `CAND_DIAG_CODE.1008: Other`,
    CDC_1010 = `CAND_DIAG_CODE.1010: Other`,
    CDC_1049 = `CAND_DIAG_CODE.1049: Dilated Cardiomyopathy`,
    CDC_1050 = `CAND_DIAG_CODE.1050: Restrictive Cardiomyopathy`,
    CDC_1052 = `CAND_DIAG_CODE.1052: Restrictive Cardiomyopathy`,
    CDC_1054 = `CAND_DIAG_CODE.1054: Restrictive Cardiomyopathy`,
    CDC_1099 = `CAND_DIAG_CODE.1099: Restrictive Cardiomyopathy`,
    CDC_1200 = `CAND_DIAG_CODE.1200: Other`,
    CDC_1201 = `CAND_DIAG_CODE.1201: Hypertrophic Cardiomyopathy`,
    CDC_1202 = `CAND_DIAG_CODE.1202: Valvular Heart Disease`,
    CDC_1203 = `CAND_DIAG_CODE.1203: Other`,
    CDC_1204 = `CAND_DIAG_CODE.1204: Other`,
    CDC_1205 = `CAND_DIAG_CODE.1205: Congenital Heart Disease Without Surgery`,
    CDC_1206 = `CAND_DIAG_CODE.1206: Congenital Heart Disease Without Surgery`,
    CDC_1207 = `CAND_DIAG_CODE.1207: Congenital Heart Disease With Surgery`,
    CDC_1208 = `CAND_DIAG_CODE.1208: Other`,
    CDC_1209 = `CAND_DIAG_CODE.1209: Dilated Cardiomyopathy`,
    CDC_999_Other = `CAND_DIAG_CODE.999: Other`,
    CDC_999_Dilated_Cardiomyopathy = `CAND_DIAG_CODE.999: Dilated Cardiomyopathy`
  )

# Rename columns in the test dataset
X_test_filtered_ohe %<>% 
  rename(
    #CDL_CHD_With_Surgery = `CAND_DIAG_LISTING.Congenital Heart Disease With Surgery`,
    #CDL_CHD_Without_Surgery = `CAND_DIAG_LISTING.Congenital Heart Disease Without Surgery`,
    #CDL_Dilated_Cardiomyopathy = `CAND_DIAG_LISTING.Dilated Cardiomyopathy`,
    #CDL_Hypertrophic_Cardiomyopathy = `CAND_DIAG_LISTING.Hypertrophic Cardiomyopathy`,
    #CDL_Myocarditis = `CAND_DIAG_LISTING.Myocarditis`,
    #CDL_Other = `CAND_DIAG_LISTING.Other`,
    #CDL_Restrictive_Cardiomyopathy = `CAND_DIAG_LISTING.Restrictive Cardiomyopathy`,
    #CDL_Valvular_Heart_Disease = `CAND_DIAG_LISTING.Valvular Heart Disease`,
    CDC_1000 = `CAND_DIAG_CODE.1000: Dilated Cardiomyopathy`,
    CDC_1001 = `CAND_DIAG_CODE.1001: Dilated Cardiomyopathy`,
    CDC_1002 = `CAND_DIAG_CODE.1002: Dilated Cardiomyopathy`,
    CDC_1003 = `CAND_DIAG_CODE.1003: Dilated Cardiomyopathy`,
    CDC_1004 = `CAND_DIAG_CODE.1004: Myocarditis`,
    CDC_1005 = `CAND_DIAG_CODE.1005: Dilated Cardiomyopathy`,
    CDC_1006 = `CAND_DIAG_CODE.1006: Myocarditis`,
    CDC_1007 = `CAND_DIAG_CODE.1007: Dilated Cardiomyopathy`,
    CDC_1008 = `CAND_DIAG_CODE.1008: Other`,
    CDC_1010 = `CAND_DIAG_CODE.1010: Other`,
    CDC_1049 = `CAND_DIAG_CODE.1049: Dilated Cardiomyopathy`,
    CDC_1050 = `CAND_DIAG_CODE.1050: Restrictive Cardiomyopathy`,
    CDC_1052 = `CAND_DIAG_CODE.1052: Restrictive Cardiomyopathy`,
    CDC_1054 = `CAND_DIAG_CODE.1054: Restrictive Cardiomyopathy`,
    CDC_1099 = `CAND_DIAG_CODE.1099: Restrictive Cardiomyopathy`,
    CDC_1200 = `CAND_DIAG_CODE.1200: Other`,
    CDC_1201 = `CAND_DIAG_CODE.1201: Hypertrophic Cardiomyopathy`,
    CDC_1202 = `CAND_DIAG_CODE.1202: Valvular Heart Disease`,
    CDC_1203 = `CAND_DIAG_CODE.1203: Other`,
    CDC_1204 = `CAND_DIAG_CODE.1204: Other`,
    CDC_1205 = `CAND_DIAG_CODE.1205: Congenital Heart Disease Without Surgery`,
    CDC_1206 = `CAND_DIAG_CODE.1206: Congenital Heart Disease Without Surgery`,
    CDC_1207 = `CAND_DIAG_CODE.1207: Congenital Heart Disease With Surgery`,
    CDC_1208 = `CAND_DIAG_CODE.1208: Other`,
    CDC_1209 = `CAND_DIAG_CODE.1209: Dilated Cardiomyopathy`,
    CDC_999_Other = `CAND_DIAG_CODE.999: Other`,
    CDC_999_Dilated_Cardiomyopathy = `CAND_DIAG_CODE.999: Dilated Cardiomyopathy`
  )

# Review the structure of the updated dataframes
str(X_train_filtered_ohe)

```

```{python}
#| echo: false
#| warning: false
#| message: false

import numpy as np
from catboost import Pool

# initialize data
X_train_one_hot_all = r.X_train_filtered_ohe
Y = np.array(r.y_train_ohe)


X_test_one_hot_all = r.X_test_filtered_ohe
Y_test = np.array(r.y_test_ohe)

```

#### Optuna Hyperparameter Optimization - One Hot All

```{python optuna-one-hot-all, eval=FALSE}

#| eval: false
#| echo: false
#| warning: false
#| message: false

import numpy as np
from sklearn.model_selection import train_test_split
import pandas as pd
import catboost as cb
from catboost.utils import eval_metric
import optuna


def objective(trial):
    # Parameter suggestions
    params = {
        "objective": "Logloss",
        "eval_metric":"AUC",
        "iterations": 1000,
        "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3),
        "depth": trial.suggest_int("depth", 1, 9),
        "colsample_bylevel": trial.suggest_float("colsample_bylevel", 0.05, 1.0),
        "min_data_in_leaf": trial.suggest_int("min_data_in_leaf", 1, 100),
        "l2_leaf_reg": trial.suggest_float("l2_leaf_reg", 1, 12),
        "boosting_type": "Ordered",
        "bootstrap_type": "MVS",
        "early_stopping_rounds": 100
    }

    model = cb.CatBoostClassifier(**params)
    train_pool = cb.Pool(X_train_one_hot_all, Y)
    cv_results = cb.cv(train_pool, params, fold_count=3, seed=3590, stratified=True, verbose=False)
    return np.max(cv_results['test-AUC-mean'])

study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=25, timeout=6000)

print("Number of finished trials: {}".format(len(study.trials)))
print("Best trial:")
for key, value in study.best_trial.params.items():
    print("  {}: {}".format(key, value))


```

```{python optuna-params4}

model_params4 = {
    'learning_rate': 0.208,
    'depth': 3,
    'colsample_bylevel': 0.35,
    'min_data_in_leaf': 14,
    'l2_leaf_reg': 9.07
}
    
```

```{python catboost-model-one-hot-encoding-all}

#| eval: true
#| echo: true
#| message: false
#| warning: false

from sklearn.model_selection import train_test_split
from catboost import CatBoostClassifier, Pool


one_hot_all = CatBoostClassifier(iterations=8000,
                               objective='Logloss',
                               eval_metric='AUC',
                               **model_params4, 
                               boosting_type= 'Ordered',
                               metric_period=500,
                               bootstrap_type='MVS',
                               early_stopping_rounds=100,
                               use_best_model=True, 
                               random_seed=1997)
                               

# Create a Pool object for the training and testing data
train_pool = Pool(X_train_one_hot_all, Y)
test_pool = Pool(X_test_one_hot_all, Y_test)
 

one_hot_all.fit(train_pool, eval_set=test_pool)

gain_one_hot_all = one_hot_all.get_feature_importance(prettified=True)
loss_one_hot_all = one_hot_all.get_feature_importance(test_pool, type='LossFunctionChange', prettified=True)

```

#### Feature Importances: One Hot Encoding All

```{r feature-importance-one-hot}
#| echo: false
#| layout-ncol: 1
#| label: tbl-feature-importance-one-hot-all
#| tbl-cap: "CatBoost Feature Importance with One-Hot Encoding"
#| tbl-subcap: 
#|   - "Gain"
#|   - "Loss Function Change"
#| warning: false
#| message: false
#| eval: true

gain_tbl_ohe_all <- py$gain_one_hot_all
loss_tbl_ohe_all <- py$loss_one_hot_all

gain_table_ohe_all <- tibble( 'Feature ID' = gain_tbl_ohe_all$`Feature Id`,
                  'Importance' = gain_tbl_ohe_all$Importances) %>% 
  rowid_to_column(var = "Rank") %>% 
  as_hux() %>%
  theme_article() %>% 
  set_align(col=c('Rank','Importance'), value= "center") %>% 
  set_tb_padding(2)


loss_table_ohe_all <- tibble( 'Feature ID' = loss_tbl_ohe_all$`Feature Id`,
                  'Importance' = loss_tbl_ohe_all$Importances) %>% 
  rowid_to_column(var = "Rank") %>% 
  as_hux() %>%
  theme_article() %>% 
  set_align(col=c('Rank','Importance'), value= "center") %>% 
  set_tb_padding(2)

gain_table_ohe_all
loss_table_ohe_all

```

#### Model Metrics

```{python}
#| echo: false
#| warning: false
#| message: false

from sklearn.metrics import roc_auc_score, brier_score_loss, accuracy_score, log_loss
import pandas as pd

Y_Pred_OHE_All = one_hot_all.predict(X_test_one_hot_all)

Y_Pred_Proba_Positive_OHE_All = one_hot_all.predict_proba(X_test_one_hot_all)[:, 1]  # Probabilities of the positive class

# Calculate AUC
auc = roc_auc_score(Y_test, Y_Pred_Proba_Positive_OHE_All)
# Calculate Brier Score
brier_score = brier_score_loss(Y_test, Y_Pred_Proba_Positive_OHE_All)
# Calculate Accuracy
accuracy = accuracy_score(Y_test, Y_Pred_OHE_All)
# Calculate Log Loss
log_loss_value = log_loss(Y_test, Y_Pred_Proba_Positive_OHE_All)

# Create a DataFrame
CB_one_hot_all = pd.DataFrame({
    'Model': ['Catboost_OHE_All'],
    'AUC': [auc],
    'Brier Score': [brier_score],
    'Accuracy': [accuracy],
    'Log Loss': [log_loss_value]
})

print(CB_one_hot_all)


```

### XGBoost

```{r}
#| echo: true
#| warning: false
#| message: false

# Combine features and labels into training and testing dataframes
model_train <- cbind(X_train_filtered_ohe, Outcome = as.factor(y_train_ohe))
model_test <- cbind(X_test_filtered_ohe, Outcome = as.factor(y_test_ohe))

```

#### Optuna Hyperparameter Optimization

```{r optuna-xgboost-r, eval=FALSE}
#| echo: true
#| warning: false
#| message: false
#| eval: false

library(reticulate)
library(parsnip)
library(dplyr)
library(yardstick)

# Import Optuna from Python
optuna <- import("optuna")

# Define the objective function
objective <- function(trial) {
  # Define the hyperparameters you want to tune
  trees <- trial$suggest_int("trees", 1, 250)
  tree_depth <- trial$suggest_int("tree_depth", 1, 12)
  learn_rate <- trial$suggest_loguniform("learn_rate", 0.01, 0.1)
  min_n <- trial$suggest_int("min_n", 1, 10)
  sample_size <- trial$suggest_uniform("sample_size", 0.5, 1.0)
  mtry <- trial$suggest_int("mtry", 1, 10)
  
  # Define the XGBoost model specification with the suggested hyperparameters
  xgb_spec <- boost_tree(
    trees = trees,
    tree_depth = tree_depth,
    learn_rate = learn_rate,
    min_n = min_n,
    sample_size = sample_size,
    mtry = mtry,
    engine = "xgboost"
  ) %>%
    set_mode("classification")
  
  # Fit the model on training data
  xgb_fit <- fit(
    xgb_spec,
    Outcome ~ .,
    data = model_train
  )
  
  # Make predictions on the validation set
  predictions <- predict(xgb_fit, new_data = model_test, type = "prob") %>%
    bind_cols(predict(xgb_fit, new_data = model_test, type = "class")) %>%
    dplyr::mutate(truth = model_test$Outcome)
  
  # Check the structure of the predictions dataframe
  print(str(predictions))
  
  # Calculate the evaluation metric (e.g., log loss)
  log_loss <- mn_log_loss(predictions, truth = truth, .pred_1)
  
  # Return the metric value as a numeric
  return(log_loss$.estimate)
}

# Create an Optuna study
study <- optuna$create_study(direction = "minimize")

# Optimize the study
study$optimize(objective, n_trials = 500)

# Get the best hyperparameters
best_params <- study$best_params
print(best_params)

```

Best is trial #494/500 with value: .699 for log loss

```{r}

xgb_best_params <- list(
  trees = 1,
  tree_depth = 5,
  learn_rate = 0.01,
  min_n = 6,
  sample_size = 0.587,
  mtry = 5
)

```

```{r xgboost-model}
#| echo: true
#| warning: false
#| message: false
#| eval: true

library(parsnip)
library(yardstick)


# XGBoost model specification
xgb_spec <- boost_tree(
  trees = xgb_best_params$trees,
  tree_depth = xgb_best_params$tree_depth,
  min_n = xgb_best_params$min_n,
  sample_size = xgb_best_params$sample_size,
  mtry = xgb_best_params$mtry,
  learn_rate = xgb_best_params$learn_rate,
  loss_reduction = 0.01,
  engine = "xgboost"
) %>%
  set_mode("classification")

# Fit the model
xgb_fit <- fit(
  xgb_spec,
  Outcome ~ .,
  data = model_train
)

# Make predictions on the test data
xgb_predictions <- predict(xgb_fit, new_data = model_test, type = "prob") %>%
  bind_cols(predict(xgb_fit, new_data = model_test)) %>%
  dplyr::mutate(truth = model_test$Outcome)

```

#### Model Metrics

```{r xgboost-model-metrics}
#| echo: false
#| warning: false
#| message: false
#| eval: true

# Evaluate the model performance
xgb_accuracy <- xgb_predictions %>%
  metrics(truth = truth, estimate = .pred_class) %>%
  filter(.metric == "accuracy") %>%
  pull(.estimate)

xgb_brier_score <- mean((xgb_predictions$.pred_1 - as.numeric(xgb_predictions$truth) - 1) ^ 2)

xgb_roc_auc <- xgb_predictions %>%
  roc_auc(truth = truth, .pred_1) %>%
  pull(.estimate)

xgb_log_loss <- xgb_predictions %>%
  mn_log_loss(truth = truth, .pred_1) %>%
  pull(.estimate)


```

```{python}
#| echo: true
#| warning: false
#| message: false
#| eval: true

# Create a DataFrame
xgboost = pd.DataFrame({
    'Model': ['XGBoost'],
    'AUC': [r.xgb_roc_auc],
    'Brier Score': [r.xgb_brier_score],
    'Accuracy': [r.xgb_accuracy],
    'Log Loss': [r.xgb_log_loss]
})

print(xgboost)

```

### Random Forest

```{r rf-setup}
#| echo: true
#| warning: false
#| message: false

# Function to summarize missing data
summarize_missing_data <- function(data) {
  data %>%
    summarise_all(~ sum(is.na(.))) %>%
    gather(key = "column", value = "num_missing") %>%
    filter(num_missing > 0) %>%
    arrange(desc(num_missing))
}

# Summarize missing data in training and testing dataframes
missing_train <- summarize_missing_data(model_train)
missing_test <- summarize_missing_data(model_test)

print("Missing data in training set:")
print(missing_train)

print("Missing data in testing set:")
print(missing_test)

```

```{r rf-preprocessing}
#| echo: false
#| warning: false
#| message: false

# Drop columns with more than 500 missing values
columns_to_drop <- missing_train %>%
  filter(num_missing > 500) %>%
  pull(column)

model_train <- model_train %>%
  select(-one_of(columns_to_drop))

model_test <- model_test %>%
  select(-one_of(columns_to_drop))

# Verify columns are dropped
print("Columns dropped from training and testing sets:")
print(columns_to_drop)

```

```{r rf-preprocessing-cont}
#| echo: false
#| warning: false
#| message: false

# Drop rows with any remaining missing values in training and testing sets
model_train <- model_train %>%
  drop_na()

model_test <- model_test %>%
  drop_na()

# Verify there are no more missing values
print("Missing data after dropping rows:")
print(summarize_missing_data(model_train))
print(summarize_missing_data(model_test))

```

#### Optuna Hyperparameter Optimization

```{r optuna-random-forest}
#| echo: true
#| warning: false
#| message: false
#| eval: false

library(reticulate)
library(parsnip)
library(dplyr)
library(yardstick)

# Import Optuna from Python
optuna <- import("optuna")

# Define the objective function
objective <- function(trial) {
  # Define the hyperparameters you want to tune
  trees <- trial$suggest_int("trees", 1, 1000)
  min_n <- trial$suggest_int("min_n", 1, 10)
  mtry <- trial$suggest_int("mtry", 1, 10)
  
  # Define the XGBoost model specification with the suggested hyperparameters
  rf_spec <- rand_forest(
    trees = trees,
    min_n = min_n,
    mtry = mtry,
    engine = "randomForest"
  ) %>%
    set_mode("classification")
  
  # Fit the model on training data
  rf_fit <- fit(
    rf_spec,
    Outcome ~ .,
    data = model_train
  )
  
  # Make predictions on the validation set
  predictions <- predict(rf_fit, new_data = model_test, type = "prob") %>%
    bind_cols(predict(rf_fit, new_data = model_test, type = "class")) %>%
    dplyr::mutate(truth = model_test$Outcome)
  
  # Check the structure of the predictions dataframe
  print(str(predictions))

# Calculate the evaluation metric (e.g., log loss)
  log_loss <- mn_log_loss(predictions, truth = truth, .pred_1)
  
  # Return the metric value as a numeric
  return(log_loss$.estimate)
}

# Create an Optuna study
study <- optuna$create_study(direction = "minimize")

# Optimize the study
study$optimize(objective, n_trials = 500)

# Get the best hyperparameters
best_params <- study$best_params
print(best_params)


```

Best is trial #172/500 with value: 2.67.

```{r}

rf_best_params <- list(
  trees = 986,
  min_n = 1,
  mtry = 10
)

```

```{r}
#| echo: true
#| warning: false
#| message: false

# Fit the final model with best hyperparameters
final_rf_spec <- rand_forest(
  trees = rf_best_params$trees,
  min_n = rf_best_params$min_n,
  mtry = rf_best_params$mtry
) %>%
  set_engine("randomForest") %>%
  set_mode("classification")

final_rf_fit <- fit(
  final_rf_spec,
  Outcome ~ .,
  data = model_train
)

# Predict on the test set
rf_predictions <- predict(final_rf_fit, new_data = model_test, type = "prob") %>%
  bind_cols(predict(final_rf_fit, new_data = model_test)) %>%
  dplyr::mutate(truth = model_test$Outcome)


```

#### Model Metrics

```{r rf-model-metrics}
#| echo: false
#| warning: false
#| message: false

# Evaluate the model performance
rf_accuracy <- rf_predictions %>%
  metrics(truth = truth, estimate = .pred_class) %>%
  filter(.metric == "accuracy") %>%
  pull(.estimate)

rf_brier_score <- mean((rf_predictions$.pred_1 - as.numeric(rf_predictions$truth) - 1) ^ 2)

rf_roc_auc <- rf_predictions %>%
  roc_auc(truth = truth, .pred_1) %>%
  pull(.estimate)

rf_log_loss <- rf_predictions %>%
  mn_log_loss(truth = truth, .pred_1) %>%
  pull(.estimate)


```

```{python}
#| echo: false
#| warning: false
#| message: false

# Create a DataFrame
random_forest = pd.DataFrame({
    'Model': ['RandomForest'],
    'AUC': [r.rf_roc_auc],
    'Brier Score': [r.rf_brier_score],
    'Accuracy': [r.rf_accuracy],
    'Log Loss': [r.rf_log_loss]
})

print(random_forest)

```

### Oblique Random Forest

-   Training Data

```{r oarfs-preprocessing}
#| echo: false
#| warning: false
#| message: false

# Missing values per column
missing_per_column <- colSums(is.na(survival_model))

# Identify columns to drop (those with over 500 missing values)
columns_to_drop <- names(missing_per_column[missing_per_column > 500])

# Drop identified columns
survival_model_cleaned <- survival_model %>%
  select(-one_of(columns_to_drop))

# Print the columns that were dropped
cat("Columns dropped due to excessive missing values:\n")
print(columns_to_drop)


```

```{r}
#| echo: false
#| warning: false
#| message: false

# Drop rows with any remaining missing values
survival_model_cleaned <- survival_model_cleaned %>%
  mutate(outcome = as.factor(outcome)) %>% 
  drop_na()

```

-   Validation Data

```{r}
#| echo: false
#| warning: false
#| message: false

# Missing values per column
missing_per_column <- colSums(is.na(survival_model_test))

# Identify columns to drop (those with over 500 missing values)
columns_to_drop <- names(missing_per_column[missing_per_column > 500])

# Drop identified columns
survival_model_test_cleaned <- survival_model_test %>%
  select(-one_of(columns_to_drop))

# Print the columns that were dropped
cat("Columns dropped due to excessive missing values:\n")
print(columns_to_drop)


```

```{r}
#| echo: false
#| warning: false
#| message: false

# Drop rows with any remaining missing values
survival_model_test_cleaned <- survival_model_test_cleaned %>%
  mutate(outcome = as.factor(outcome)) %>% 
  drop_na()

```

```{r}
#| echo: false
#| warning: false
#| message: false

survival_model_ <- rbind(survival_model_cleaned, survival_model_test_cleaned)

```

```{r aorf-model}
#| echo: true
#| warning: false
#| message: false

library(aorsf)

# An oblique classification RF
aorf_model <- orsf(data = survival_model_,
                    n_tree = 100, 
                    formula = outcome ~ .)

aorf_model

```

#### Feature Importances

```{r aorf-feature-importance}
#| echo: true
#| warning: false
#| message: false

 aorf_feature_importances <- orsf_vi_anova(aorf_model) 

# Create a dataframe with named columns (feature) sorted by value (Importance)
aorf_feature_importances_df <- data.frame(
  Feature = names(aorf_feature_importances),
  Importance = as.numeric(aorf_feature_importances)
) %>%
  arrange(desc(Importance))

# Print the feature importances dataframe
print(aorf_feature_importances_df)

```

```{r echo=FALSE}
#| echo: true
#| warning: false
#| message: false

aorf_predictions <- aorf_model$pred_oobag

aorf_predict <- cbind(aorf_predictions, survival_model_["outcome"])

aorf_roc_auc <- aorf_model$eval_oobag$stat_values

```


```{r echo=FALSE}
#| echo: true
#| warning: false
#| message: false

colnames(aorf_predict) <- c("prob_0", "prob_1", "truth")

aorf_predict <- aorf_predict %>%
  mutate(truth = as.factor(truth),
         prob_0 = as.numeric(prob_0),
         prob_1 = as.numeric(prob_1))

```

```{r}
#| echo: true
#| warning: false
#| message: false

# Calculate the ROC curve
roc_curve <- pROC::roc(aorf_predict$truth, aorf_predict$prob_1, levels = rev(levels(aorf_predict$truth)))

# Calculate the optimal threshold using Youden's J statistic
optimal_threshold <- pROC::coords(roc_curve, "best", ret = "threshold")

cat("Optimal Threshold:", optimal_threshold$threshold, "\n")

```

```{r}
#| echo: false
#| warning: false
#| message: false

# Convert probabilities to class labels based on the optimal threshold
aorf_predict <- aorf_predict %>%
  mutate(pred_class = ifelse(prob_1 > optimal_threshold$threshold, 1, 0))

# Ensure truth and pred_class are factors
aorf_predict <- aorf_predict %>%
  mutate(pred_class = as.factor(pred_class))

```

```{r}
#| echo: false
#| warning: false
#| message: false

# Evaluate the model performance
aorf_accuracy <- aorf_predict %>%
  metrics(truth = truth, estimate = pred_class) %>%
  filter(.metric == "accuracy") %>%
  pull(.estimate)

aorf_roc_auc <- aorf_predict %>%
  roc_auc(truth = truth, prob_1) %>%
  pull(.estimate)

aorf_log_loss <- aorf_predict %>%
  mn_log_loss(truth = truth, prob_1) %>%
  pull(.estimate)

aorf_brier_score <- aorf_predict %>%
  brier_class(truth = truth, prob_1) %>%
  pull(.estimate)


```

```{python}
#| echo: false
#| warning: false
#| message: false

# Create a DataFrame
ao_random_forest = pd.DataFrame({
    'Model': ['AcceleratedObliqueRandomForest'],
    'AUC': [r.aorf_roc_auc],
    'Brier Score': [r.aorf_brier_score],
    'Accuracy': [r.aorf_accuracy],
    'Log Loss': [r.aorf_log_loss]
})

print(ao_random_forest)

```

### Final Model Accuracy Metrics

```{r}
#| echo: false
#| warning: false
#| message: false

final_model_accuaracy_metrics <- rbind(py$catboost_native, py$feature_importance_model_df, py$candidate_diagnosis_one_hot,  py$CB_one_hot_all, py$xgboost, py$random_forest, py$ao_random_forest )


final_model_accuaracy_metrics

```

### SHAP Value Analysis for Best Model

```{python shap-setup}

#| eval: true
#| echo: false
#| message: false
#| warning: false

import shap

explainer = shap.TreeExplainer(one_hot_all)
shap_values = explainer(X_train_one_hot_all)
    
```

#### Beeswarm (Top Features - Categorical and Numerical)

```{python}
#| eval: false
#| echo: false
#| message: false
#| warning: false

mean_abs_shap_values = np.mean(np.abs(shap_values.values), axis=0)

top_n = 25
top_indices = np.argsort(-mean_abs_shap_values)[:top_n]  # Indices of the top features

filtered_shap_values = shap_values[:, top_indices]

```

```{python shap-setup2}
#| eval: false
#| echo: false
#| message: false
#| warning: false

plt.close()

import matplotlib.pyplot as plt
import shap

shap.initjs()


shap.plots.beeswarm(filtered_shap_values,20)

plt.xlabel('', fontsize=8)
plt.ylabel('', fontsize=8)
plt.title('', fontsize=8)
plt.tight_layout()

plt.savefig('images/shapley_beeswarm.png', dpi=1200)

```

![Beeswarm Chart](images/shapley_beeswarm.png){#fig-shap-beeswarm-chart}

#### Bar Chart for Feature Importance

```{python shapley-bar-chart}
#| eval: false
#| echo: false
#| warning: false
#| message: false
#| label: fig-net-effect
#| fig-cap: 
#|   - "Net Effect"

import matplotlib.pyplot as plt
import shap

shap.initjs()

plt.close()
plt.close()

shap.plots.bar(filtered_shap_values, max_display=20)

plt.tight_layout()

# Save the image in high resolution
plt.savefig('images/shapley_bar_one_hot.png', dpi=1200)

```

![Feature Importance Chart](images/shapley_bar_one_hot.png){#fig-shap-bar-chart}

```{python}
#| eval: false
#| echo: false
#| warning: false
#| message: false


# Get the list of all columns
all_columns = X_train_one_hot_all.columns.tolist()

# Function to check if a column is one-hot encoded
def is_one_hot_encoded(column):
    unique_values = X_train_one_hot_all[column].unique()
    return set(unique_values).issubset({0, 1}) and len(unique_values) <= 2

# Select numerical features that are not one-hot encoded
num_features = [col for col in all_columns if X_train_one_hot_all[col].dtype in ['int64', 'float64'] and not is_one_hot_encoded(col)]
print("Numerical Features:", num_features)

# Get the indices of numerical features
num_feature_indices = [X_train_one_hot_all.columns.get_loc(col) for col in num_features]
print("Indices of Numerical Features:", num_feature_indices)


```

#### Beeswarm Top Numerical Features

```{python beeswarm-numerical-features}
#| eval: false
#| echo: false
#| warning: false
#| message: false
#| out-width: 70%
#| out-height: 70%
#| fig-cap: 
#|   - "Beeswarm Numerical Effects"

import matplotlib.pyplot as plt
import shap
import pickle

shap.initjs()

plt.close()
plt.close()


# Subset the SHAP values to only include the numerical features
numerical_shap_values = shap_values[:, num_feature_indices]

# Plot the beeswarm for only the numerical features
shap.plots.beeswarm(numerical_shap_values)

plt.tight_layout()

# Save the figure with desired DPI
plt.savefig('images/shapley_beeswarm_numerical.png', dpi=1200, bbox_inches="tight")

```

![Numerical Beeswarm Chart](images/shapley_beeswarm_numerical.png){#fig-shap-numerical-beeswarm-chart}

#### SHAP Value Feature Importance Correlation Plot

```{python correlation-plot}
#| eval: false
#| echo: false
#| warning: false
#| message: false
#| out-width: 70%
#| out-height: 70%
#| fig-cap: 
#|   - "SHAP Value Correlation Plot"

plt.close()

import seaborn as sns
import matplotlib.pyplot as plt

# SHAP correlation plot
corr_matrix = pd.DataFrame(numerical_shap_values.values, columns=num_features).corr()

sns.set(font_scale=.5)
sns.heatmap(corr_matrix,cmap="coolwarm", center=0, annot=True, fmt =".1g")

plt.tight_layout()

# Save the figure with desired DPI
plt.savefig('images/shap_correlation_plot.png', dpi=1200, bbox_inches="tight")

```

![SHAP Value Correlation Plot](images/shap_correlation_plot.png){#fig-shap-correlation-plot}

#### SHAP Value Feature Importance - Mean Absolute Value

```{r shap-values}
#| echo: false
#| warning: false
#| message: false
#| eval: true

# SHAP values
shap_values <- py$shap_values

# Extract SHAP values for each feature (excluding the last column which is the expected value)
shap_values_matrix <- shap_values$values

# Convert SHAP values to a dataframe for easier analysis
shap_df <- as.data.frame(shap_values_matrix)
names(shap_df) <- shap_values$feature_names

```

```{r mean-absolute-shap-values}
#| echo: false
#| warning: false
#| message: false
#| eval: true

features <- names(py$X_train_one_hot_all)

mean_abs_shap_values <- colMeans(abs(shap_df))  # Compute mean absolute SHAP values

```

```{r radar-chart}
#| echo: true
#| warning: false
#| message: false
#| eval: true

library(plotly)

# Function to generate plot based on a threshold
generate_radar_plot <- function(threshold) {
  indices <- which(mean_abs_shap_values > threshold)
  data <- data.frame(
    r = mean_abs_shap_values[indices],
    theta = features[indices]
  )

  p <- plot_ly(data, type = 'scatterpolar', fill = 'toself',
               r = ~r, theta = ~theta, mode = 'lines+markers',
               marker = list(size = 5)) %>%
    layout(
      polar = list(
        radialaxis = list(
          visible = T,
          range = c(0, max(data$r))
        )
      )
    )
  return(p)
}

# Plot
threshold <- .015
plot <- generate_radar_plot(threshold)


# Print the plot
plot

```

#### SHAP Value Radar Chart Feature Importance - Mean Positive and Mean Negative Values

Negative SHAP values in a binary classification where "1" is positive indicate that the feature decreases the probability of the positive outcome.

Low values for Median \# Refusals and low values for Median Wait Days - pushes prediction to the '0' class - 'Survival'.

This visualization represents the positive and negative values for the 'Positive' Class ('1'). For situations that are based on both clasess (ie. One hot encoded values where SHAP value refers to the presence or absence of value) a Beeswarm chart is more appropriate. (See Dilated Cardiomyopathy for example - high positive SHAP value for absence of feature)

```{r pos-neg-shap-values}
#| echo: true
#| warning: false
#| message: false
#| eval: true

# Model Features
features <- names(shap_df)

# Mean SHAP values for each feature
mean_shap_values <- colMeans(shap_df)

# Separate positive and negative SHAP values
positive_shap_values <- mean_shap_values
negative_shap_values <- mean_shap_values

positive_shap_values[positive_shap_values < 0] <- 0
negative_shap_values[negative_shap_values > 0] <- 0

# Make negative SHAP values positive for visualization purposes
negative_shap_values <- abs(negative_shap_values)

```

```{r radar-chart-positive-negative}
#| echo: true
#| warning: false
#| message: false
#| eval: true

# Function to generate radar plot with positive and negative SHAP values
generate_radar_plot_pos_neg <- function(threshold) {
  # Filter features based on the threshold for mean absolute SHAP values
  indices <- which(colMeans(abs(shap_df)) > threshold)
  
  # Prepare data for plotly
  data_positive <- data.frame(
    r = positive_shap_values[indices],
    theta = features[indices],
    group = "Positive SHAP Values"
  )
  
  data_negative <- data.frame(
    r = negative_shap_values[indices],
    theta = features[indices],
    group = "Negative SHAP Values"
  )
  
  # Combine the data
  data_plot <- rbind(data_positive, data_negative)
  
  # Create the radar plot
  p <- plot_ly(data_plot, type = 'scatterpolar', fill = 'toself',
               r = ~r, theta = ~theta, color = ~group,
               mode = 'lines+markers',
               marker = list(size = 5)) %>%
    layout(
      polar = list(
        radialaxis = list(
          visible = TRUE,
          range = c(0, max(data_plot$r))
        )
      )
    )
  return(p)
}

# Define a threshold for SHAP value significance
threshold <- 0.015

# Generate and print the plot
plot <- generate_radar_plot_pos_neg(threshold)
plot

```
