---
title: "Survival Analysis - Binary Classification"
author: "R. Jerome Dixon"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
format:
  html:
    toc: true
    toc-depth: 5
    code-fold: true
    code-summary: "Show the code"
    embed-resources: true
    default-image-extension: svg
    dpi: 600
---

```{r load-libraries}
#| echo: true
#| warning: false
#| message: false
#| results: hide

library(here)
library(dplyr)
library(readr)
library(magrittr)
library(spatstat)
library(tibble)
library(ggplot2)
library(purrr)
library(tidyverse)
library(huxtable)
library(reticulate)
library(DT)
library(caret)
library(forcats)
library(jsonlite)

(options)(scipen=999)

```

### Survival Model Dataset

```{r load-dataset}
#| echo: true
#| warning: false
#| message: false
#| eval: true

set.seed(1997)

survival_model <- read_rds(here("data","survival_analysis.rds"))

survival_model %<>%
  mutate_if(is.character, as.factor) 
 
survival_model %>% 
  str()


```

####  Dataset

```{r format-dataset}

#| echo: true
#| warning: false
#| message: false
#| eval: true

survival_model %<>%
  mutate(INIT_STAT = as.factor(INIT_STAT)) %>%
  mutate(INOTROPES_TCR = as.factor(INOTROPES_TCR)) %>%
  mutate(PGE_TCR = as.factor(PGE_TCR)) %>%
  mutate_if(is.factor, ~fct_na_value_to_level(., "Unknown")) %>% 
  select(-c( days_total, WL_ID_CODE, PT_CODE)) %>% 
  select(-c(BMI, PGE_TCR, WL_OTHER_ORG, LISTING_CTR_CODE )) %>% 
  filter(!is.na(outcome_final))

survival_model %>% 
  str()

```

- Target Label is 'outcome_final'

```{r}

survival_model %<>% 
  rename(
    `Blood Type` = ABO,
    `Gender` = GENDER,
    `Race` = RACE,
    `Diabetes` = DIAB,
    `Height` = HEIGHT_CM,
    `Weight` = WEIGHT_KG,
    `ECMO at Reg` = ECMO_CAND_REG,
    `VAD Device TCR` = VAD_DEVICE_TY_TCR,
    `Ventilator at Reg` = VENTILATOR_CAND_REG,
    `Defribilator` = IMPL_DEFIBRIL,
    `Recent Creatinine` = MOST_RCNT_CREAT,
    `Inotropes TCR` = INOTROPES_TCR,
    `Functional Status at Registration` = FUNC_STAT_CAND_REG,
    `Total Albumin Level` = TOT_SERUM_ALBUM,
    `Candidate Diagnosis` = CAND_DIAG,
    `LC Average Volume of Txp/Year` = LISTING_CTR_TX_AVG,
    `Initial Status` = INIT_STAT,
    `Age` = AGE,
    #`LC Code` = LISTING_CTR_CODE,
    `LC Median # Offer Refusals` = median_refusals,
    `LC Median Wait Days` = median_wait_days,
    `LC Median Wait Days at Initial Status` = median_wait_days_status
    
  )

```

```{r}

cat_features <- names(survival_model)[sapply(survival_model, is.factor)]

cat_features


```

### CatBoost Encoding

```{r} 

#| echo: true
#| warning: false
#| message: false
#| eval: true

features_cb <- survival_model %>% 
  select(-outcome_final)


# Must be factor or numeric "Class"
model_target <- survival_model %>% 
  select(outcome_final) %>% 
  dummify() %>% 
  as.data.frame()

target_cb <- model_target$outcome_final

```

#### Utility Function for Categorical Indexes needed by CatBoost

```{python}

def get_categorical_indexes(X_train):
    # Select columns with object or categorical dtype
    categorical_columns = X_train.select_dtypes(include=['object', 'category'])

    # Get the column indexes of categorical variables
    categorical_indexes = [X_train.columns.get_loc(col) for col in categorical_columns]

    return categorical_indexes

```


```{python catboost-r-data}

#| echo: true
#| warning: false
#| message: false
#| results: hide

import numpy as np
from catboost import Pool

# initialize data
X = r.features_cb

y = r.target_cb

cat_index = get_categorical_indexes(X)

# Using this object for feature importance calculations
test_data = catboost_pool = Pool(X, y, cat_index)


```

#### Hyperparameter Optimization with Optuna

```{python optuna-hyperparameter-optimization, eval=FALSE}

#| eval: false
#| echo: true
#| warning: false
#| message: false
#| results: hide

import numpy as np
from sklearn.model_selection import train_test_split, StratifiedKFold
import pandas as pd
import catboost as cb
import optuna
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score

# Convert target label from list to array
Y = np.array(y)  

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=42)

def objective(trial):
    learning_rate = trial.suggest_float("learning_rate", 0.01, 0.3)
    depth = trial.suggest_int("depth", 1, 5)
    colsample_bylevel = trial.suggest_float("colsample_bylevel", 0.05, 1.0)
    min_data_in_leaf = trial.suggest_int("min_data_in_leaf", 1, 100)
    l2_leaf_reg = trial.suggest_float("l2_leaf_reg", 1, 12)
  
    params = {
        "objective": "Logloss",
        "iterations": 1000,
        "learning_rate": learning_rate,
        "depth": depth,
        "colsample_bylevel": colsample_bylevel,
        "min_data_in_leaf": min_data_in_leaf,
        "boosting_type": "Ordered",
        "bootstrap_type": "MVS",
        "l2_leaf_reg": l2_leaf_reg,
        "early_stopping_rounds": 100,
        "eval_metric": 'Logloss',
    }

    # Initialize CatBoostClassifier with parameters
    model = cb.CatBoostClassifier(**params)

    # Convert training data into a Pool to specify cat features
    train_pool = cb.Pool(X_train, Y_train, cat_features=get_categorical_indexes(X_train))

    # Perform cross-validation
    cv_results = cb.cv(train_pool, params, fold_count=3, seed=42, stratified=True, verbose=False)

    # Negative mean Logloss to maximize (since Optuna minimizes)
    mean_logloss = -np.mean(cv_results['test-Logloss-mean'])

    return mean_logloss


study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=100, timeout=6000)

print("Number of finished trials: {}".format(len(study.trials)))
print("Best trial:")
best_trial = study.best_trial

print("  Params: ")
for key, value in best_trial.params.items():
    print("    {}: {}".format(key, value))

```

```{python optuna-params}

model_params = {
    'learning_rate': 0.25,
    'depth': 1,
    'colsample_bylevel': 0.25,
    'min_data_in_leaf': 36,
    'l2_leaf_reg': 8.45
}
    
```

#### Model

```{python catboost-model-logloss}

#| eval: true
#| echo: true
#| message: false
#| warning: false
#| results: hide

import numpy as np
from sklearn.model_selection import train_test_split, StratifiedKFold
import pandas as pd
import optuna
from catboost import CatBoostClassifier, Pool
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score

# Convert target label from list to array
Y = np.array(y)  

X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, train_size=0.80, stratify=y)

cat_indexes = get_categorical_indexes(X_train)

model_logloss = CatBoostClassifier(iterations=1000,
                               objective='Logloss',
                               **model_params, 
                               boosting_type= 'Ordered',
                               bootstrap_type='MVS',
                               early_stopping_rounds=100,
                               use_best_model=True, random_seed=1997)
                               

# Create a Pool object for the training and testing data
train_pool = Pool(X_train, cat_features=cat_indexes, label=Y_train)
test_pool = Pool(X_validation, cat_features=cat_indexes, label=Y_validation)
 

model_logloss.fit(train_pool, eval_set=test_pool)

```

```.text
bestTest = 0.2799098586
bestIteration = 71
Shrink model to first 72 iterations.
```

#### Model Accuracy

##### Predictions

```{python}

import pandas as pd

Y_Pred = model_logloss.predict(X_validation)
Y_Pred_Proba = model_logloss.predict_proba(X_validation)[:, 1]  # get the probabilities of the positive class


Y_Pred_Proba_Positive = model_logloss.predict_proba(X_validation)[:, 1]  # Probabilities of the positive class
Y_Pred_Proba_Negative = model_logloss.predict_proba(X_validation)[:, 0]  # Probabilities of the negative class

# Converting predictions and actuals into a DataFrame for better readability, including negative class probabilities
predictions = pd.DataFrame({
    'Prob_Negative_Class': Y_Pred_Proba_Negative,
    'Prob_Positive_Class': Y_Pred_Proba_Positive,
    'Predicted': Y_Pred,
    'Actual': Y_validation
})

# Converting predictions and actuals into a DataFrame for better readability
predictions = pd.DataFrame({
    'Prob_Positive_Class': Y_Pred_Proba,
    'Predicted': Y_Pred,
    'Actual': Y_validation
})

```


```{r message=FALSE, warning=FALSE}

library(probably)

predictions <- py$predictions %>% 
  mutate(Class = ifelse(Actual == 0, "survive", "not_survive"),
         .pred_not_survive = Prob_Positive_Class
         )

# Define the levels you want
factor_levels <- c("survive", "not_survive")

# Set the levels of the 'actuals' column
predictions$Class <- factor(predictions$Class, levels = rev(factor_levels))

predictions %>% 
  datatable()


```

##### Calibration Plot - CatBoost

```{r}

predictions %>% 
  cal_plot_logistic(Class, .pred_not_survive)

```

##### Decision Threshold

```{r message=FALSE, warning=FALSE}

library(pROC)

# Calculate the ROC curve
roc_result <- roc(predictions$Actual, predictions$Prob_Positive_Class)

coords <- coords(roc_result, "best", ret="threshold", best.method="closest.topleft")

# Optimal threshold for maximizing true positive rate
optimal_threshold <- coords$threshold

# Apply the optimal threshold to convert probabilities to class predictions
predictions$predicted_classes <- ifelse(predictions$Prob_Positive_Class >= optimal_threshold, 1, 0)

# Output the optimal threshold
cat("Optimal Threshold:", optimal_threshold, "\n")

cat("Number of '1's predicted:", sum(predictions$predicted_classes), "\n")

```

##### Calibrated Model Accuracy Using Decision Threshold

```{python}

#| echo: true
#| warning: false
#| message: false
#| eval: false

from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, roc_curve, roc_auc_score

# Calculate metrics
f1_updated = f1_score(r.predictions["Actual"], r.predictions["predicted_classes"])
precision_updated = precision_score(r.predictions["Actual"], r.predictions["predicted_classes"])
recall_updated = recall_score(r.predictions["Actual"], r.predictions["predicted_classes"])
accuracy_updated = accuracy_score(r.predictions["Actual"], r.predictions["predicted_classes"])

# Calculate the area under the curve (AUC)
model_auc_updated = roc_auc_score(r.predictions["Actual"], r.predictions["Prob_Positive_Class"])
conf_matrix_updated = confusion_matrix(r.predictions["Actual"], r.predictions["predicted_classes"])

```

```{r save-updated-accuracy-metrics}

#| echo: true
#| warning: false
#| message: false
#| eval: false

model_f1_score <- py$f1_updated
model_recall <- py$recall_updated
model_precision <- py$precision_updated
model_accuracy <- py$accuracy_updated
model_roc_score <- py$model_auc_updated

model_accuracy_updated <- list(
  F1_Score = model_f1_score,
  Recall = model_recall,
  Precision = model_precision,
  Accuracy = model_accuracy,
  ROC_Score = model_roc_score
)

model_accuracy_updated

```

```{python model-accuracy-calibrated}

#| echo: true
#| results: hide
#| label: fig-model-accuracy-calibrated
#| fig-subcap:
#|   - "Confusion Matrix with Class Imbalance"
#| warning: false
#| message: false
#| eval: false

import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, roc_curve, roc_auc_score
import itertools


# Simplified class names for ticks
class_names = ['Class 0', 'Class 1']
# Detailed class names
detailed_class_names = ['Class 0: Transplanted', 'Class 1: WL Mortality/Deterioration']


normalize = conf_matrix_updated.astype('float') / conf_matrix_updated.sum(axis=1)[:, np.newaxis]

plt.figure(figsize=(10, 7))
plt.imshow(normalize, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix with Class Imbalance')
plt.colorbar()

tick_marks = np.arange(len(class_names))
plt.xticks(tick_marks, class_names, rotation=45)
plt.yticks(tick_marks, class_names)

fmt = '.2f'
for i, j in itertools.product(range(conf_matrix_updated.shape[0]), range(conf_matrix_updated.shape[1])):
    plt.text(j, i, format(conf_matrix_updated[i, j], 'd') + "\n" + format(normalize[i, j], fmt),
             horizontalalignment="center",
             color="white" if normalize[i, j] > 0.5 else "black")

plt.ylabel('True label')
plt.xlabel('Predicted label')

# Show class distribution (imbalance ratio)
class_counts = np.bincount(r.predictions["Actual"])
for i, count in enumerate(class_counts):
    plt.text(i, i, f"\n\nCount: {count}", horizontalalignment="center", verticalalignment="center",
             color="white" if normalize[i, i] > 0.5 else "black")


# Set the titles. Main title and then a subtitle with detailed class names.
plt.suptitle('Confusion Matrix with Class Imbalance', fontsize=16)
plt.title(detailed_class_names, fontsize=10, style='italic')

# Adjust the layout to make room for the main title
plt.tight_layout(rect=[0, 0.03, 1, 0.95]) 

# Save the image in high resolution
plt.savefig('images/model_accuracy_catboost_encoding.png', dpi=1200)

```
![Calibrated Confusion Matrix](images/model_accuracy_catboost_encoding.png){#fig-model-accuracy2}

### One Hot Encoding

```{r}
survival_model_one_hot <- survival_model
```

```{r}

# Create the dummy variables specification
dummies <- dummyVars(~ ., data = survival_model_one_hot[, cat_features], fullRank = FALSE)

# Generate the dummy variables
df_dummies <- predict(dummies, newdata = survival_model_one_hot)

# Bind the new dummy variables with the original dataframe minus the original factor columns
survival_model_one_hot <- cbind(survival_model_one_hot[, !(names(survival_model_one_hot) %in% cat_features)], df_dummies)

# Review the structure of the updated dataframe
str(survival_model_one_hot)


```

```{r}

features_cb <- survival_model_one_hot %>% 
  select(-outcome_final)

# Must be factor or numeric "Class"
model_target <- survival_model_one_hot %>% 
  select(outcome_final) %>% 
  dummify() %>% 
  as.data.frame()

target_cb <- model_target$outcome_final

```


```{python catboost-r-data}

#| echo: true
#| warning: false
#| message: false
#| results: hide

import numpy as np
from catboost import Pool

# initialize data
X = r.features_cb

y = r.target_cb

# Using this object for feature importance calculations
test_data = catboost_pool = Pool(X, y)

```

#### Hyperparameter Optimization with Optuna

```{python optuna-hyperparameter-optimization, eval=FALSE}

#| eval: false
#| echo: true
#| warning: false
#| message: false
#| results: hide

import numpy as np
from sklearn.model_selection import train_test_split, StratifiedKFold
import pandas as pd
import catboost as cb
import optuna
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score

# Convert target label from list to array
Y = np.array(y)  

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=42)

def objective(trial):
    learning_rate = trial.suggest_float("learning_rate", 0.01, 0.3)
    depth = trial.suggest_int("depth", 3, 9)
    colsample_bylevel = trial.suggest_float("colsample_bylevel", 0.05, 1.0)
    min_data_in_leaf = trial.suggest_int("min_data_in_leaf", 1, 100)
    l2_leaf_reg = trial.suggest_float("l2_leaf_reg", 1, 12)
  
    params = {
        "objective": "Logloss",
        "iterations": 1000,
        "learning_rate": learning_rate,
        "depth": depth,
        "colsample_bylevel": colsample_bylevel,
        "min_data_in_leaf": min_data_in_leaf,
        "boosting_type": "Ordered",
        "bootstrap_type": "MVS",
        "l2_leaf_reg": l2_leaf_reg,
        "early_stopping_rounds": 100,
        "eval_metric": 'Logloss',
    }

    # Initialize CatBoostClassifier with parameters
    model = cb.CatBoostClassifier(**params)

    # Convert training data into a Pool to specify cat features
    train_pool = cb.Pool(X_train, Y_train)

    # Perform cross-validation
    cv_results = cb.cv(train_pool, params, fold_count=3, seed=42, stratified=True, verbose=False)

    # Negative mean Logloss to maximize (since Optuna minimizes)
    mean_logloss = -np.mean(cv_results['test-Logloss-mean'])

    return mean_logloss


study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=100, timeout=6000)

print("Number of finished trials: {}".format(len(study.trials)))
print("Best trial:")
best_trial = study.best_trial

print("  Params: ")
for key, value in best_trial.params.items():
    print("    {}: {}".format(key, value))

```

```{python optuna-params}

model_params2 = {
    'learning_rate': 0.25,
    'depth': 3,
    'colsample_bylevel': 0.05,
    'min_data_in_leaf': 30,
    'l2_leaf_reg': 10.51
}
    
```

```{python catboost-model-logloss}

#| eval: true
#| echo: true
#| message: false
#| warning: false
#| results: hide

from sklearn.model_selection import train_test_split, StratifiedKFold
from catboost import CatBoostClassifier, Pool


X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, train_size=0.80, stratify=y)

survival_model = CatBoostClassifier(iterations=1000,
                               objective='Logloss',
                               **model_params2, 
                               boosting_type= 'Ordered',
                               bootstrap_type='MVS',
                               early_stopping_rounds=100,
                               use_best_model=True, random_seed=1997)
                               

# Create a Pool object for the training and testing data
train_pool = Pool(X_train, label=Y_train)
test_pool = Pool(X_validation, label=Y_validation)
 

survival_model.fit(train_pool, eval_set=test_pool)

gain = survival_model.get_feature_importance(prettified=True)
loss = survival_model.get_feature_importance(test_data, type='LossFunctionChange', prettified=True)

```
```.text

bestTest = 0.2888392626
bestIteration = 56

Shrink model to first 57 iterations.

```
#### Model Accuracy

##### Predictions

```{python}

import pandas as pd

Y_Pred = survival_model.predict(X_validation)
Y_Pred_Proba = survival_model.predict_proba(X_validation)[:, 1]  # get the probabilities of the positive class


Y_Pred_Proba_Positive = survival_model.predict_proba(X_validation)[:, 1]  # Probabilities of the positive class
Y_Pred_Proba_Negative = survival_model.predict_proba(X_validation)[:, 0]  # Probabilities of the negative class

# Converting predictions and actuals into a DataFrame for better readability, including negative class probabilities
predictions = pd.DataFrame({
    'Prob_Negative_Class': Y_Pred_Proba_Negative,
    'Prob_Positive_Class': Y_Pred_Proba_Positive,
    'Predicted': Y_Pred,
    'Actual': Y_validation
})

# Converting predictions and actuals into a DataFrame for better readability
predictions = pd.DataFrame({
    'Prob_Positive_Class': Y_Pred_Proba,
    'Predicted': Y_Pred,
    'Actual': Y_validation
})

```


```{r message=FALSE, warning=FALSE}

library(probably)

predictions <- py$predictions %>% 
  mutate(Class = ifelse(Actual == 0, "survive", "not_survive"),
         .pred_not_survive = Prob_Positive_Class
         )

# Define the levels you want
factor_levels <- c("survive", "not_survive")

# Set the levels of the 'actuals' column
predictions$Class <- factor(predictions$Class, levels = rev(factor_levels))

predictions %>% 
  datatable()


```

##### Calibration Plot - CatBoost

```{r}

predictions %>% 
  cal_plot_logistic(Class, .pred_not_survive)

```

##### Decision Threshold

```{r message=FALSE, warning=FALSE}

library(pROC)

# Calculate the ROC curve
roc_result <- roc(predictions$Actual, predictions$Prob_Positive_Class)

coords <- coords(roc_result, "best", ret="threshold", best.method="closest.topleft")

# Optimal threshold for maximizing true positive rate
optimal_threshold <- coords$threshold

# Apply the optimal threshold to convert probabilities to class predictions
predictions$predicted_classes <- ifelse(predictions$Prob_Positive_Class >= optimal_threshold, 1, 0)

# Output the optimal threshold
cat("Optimal Threshold:", optimal_threshold, "\n")

cat("Number of '1's predicted:", sum(predictions$predicted_classes), "\n")

```

##### Calibrated Threshold Model Accuracy Using LogLoss

```{python}

#| echo: true
#| warning: false
#| message: false
#| eval: false

from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, roc_curve, roc_auc_score

# Calculate metrics
f1_updated = f1_score(r.predictions["Actual"], r.predictions["predicted_classes"])
precision_updated = precision_score(r.predictions["Actual"], r.predictions["predicted_classes"])
recall_updated = recall_score(r.predictions["Actual"], r.predictions["predicted_classes"])
accuracy_updated = accuracy_score(r.predictions["Actual"], r.predictions["predicted_classes"])

# Calculate the area under the curve (AUC)
model_auc_updated = roc_auc_score(r.predictions["Actual"], r.predictions["Prob_Positive_Class"])
conf_matrix_updated = confusion_matrix(r.predictions["Actual"], r.predictions["predicted_classes"])

```

```{r save-updated-accuracy-metrics}

#| echo: true
#| warning: false
#| message: false
#| eval: false

model_f1_score <- py$f1_updated
model_recall <- py$recall_updated
model_precision <- py$precision_updated
model_accuracy <- py$accuracy_updated
model_roc_score <- py$model_auc_updated

model_accuracy_one_hot <- list(
  F1_Score = model_f1_score,
  Recall = model_recall,
  Precision = model_precision,
  Accuracy = model_accuracy,
  ROC_Score = model_roc_score
)

model_accuracy_one_hot

```


```{python model-accuracy-calibrated}

#| echo: true
#| results: hide
#| label: fig-model-accuracy-calibrated
#| fig-subcap:
#|   - "Confusion Matrix with Class Imbalance"
#| warning: false
#| message: false
#| eval: false

import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, roc_curve, roc_auc_score
import itertools


# Simplified class names for ticks
class_names = ['Class 0', 'Class 1']
# Detailed class names
detailed_class_names = ['Class 0: Transplanted', 'Class 1: WL Mortality/Deterioration']


normalize = conf_matrix_updated.astype('float') / conf_matrix_updated.sum(axis=1)[:, np.newaxis]

plt.figure(figsize=(10, 7))
plt.imshow(normalize, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix with Class Imbalance')
plt.colorbar()

tick_marks = np.arange(len(class_names))
plt.xticks(tick_marks, class_names, rotation=45)
plt.yticks(tick_marks, class_names)

fmt = '.2f'
for i, j in itertools.product(range(conf_matrix_updated.shape[0]), range(conf_matrix_updated.shape[1])):
    plt.text(j, i, format(conf_matrix_updated[i, j], 'd') + "\n" + format(normalize[i, j], fmt),
             horizontalalignment="center",
             color="white" if normalize[i, j] > 0.5 else "black")

plt.ylabel('True label')
plt.xlabel('Predicted label')

# Show class distribution (imbalance ratio)
class_counts = np.bincount(r.predictions["Actual"])
for i, count in enumerate(class_counts):
    plt.text(i, i, f"\n\nCount: {count}", horizontalalignment="center", verticalalignment="center",
             color="white" if normalize[i, i] > 0.5 else "black")


# Set the titles. Main title and then a subtitle with detailed class names.
plt.suptitle('Confusion Matrix with Class Imbalance', fontsize=16)
plt.title(detailed_class_names, fontsize=10, style='italic')

# Adjust the layout to make room for the main title
plt.tight_layout(rect=[0, 0.03, 1, 0.95]) 

# Save the image in high resolution
plt.savefig('images/model_accuracy_one_hot.png', dpi=1200)

```

![One-Hot Encoded Confusion Matrix](images/model_accuracy_one_hot.png){#fig-model-accuracy2}

### Feature Importance - One Hot

```{r}

gain_tbl <- py$gain
loss_tbl <- py$loss

num_features <- nrow(loss_tbl) # Total number of features

importances <- loss_tbl %>%
  mutate(
    Scaled_Importances = scale(Importances)[, 1],
    Min_Importance = min(Scaled_Importances),
    Max_Importance = max(Scaled_Importances),
    Weighted_Score = ((Scaled_Importances - Min_Importance) / (Max_Importance - Min_Importance)) * num_features
  ) %>%
  select(-Scaled_Importances, -Min_Importance, -Max_Importance)


importances %>% 
  datatable()

```


### Numerical Encoding

```{r}

cat_encoded_features <- features_cb[10:75]

for (col in names(cat_encoded_features)) {
  if (any(cat_encoded_features[[col]] == 1, na.rm = TRUE)) {
    sum_col <- sum(cat_encoded_features[[col]], na.rm = TRUE)
    cat_encoded_features <- cat_encoded_features %>%
      mutate(!!col := if_else(!!sym(col) == 1, sum_col, !!sym(col)))
  }
}

# Check the result
head(cat_encoded_features)

```


```{r}

#| eval: false
#| echo: true
#| warning: false
#| message: false
#| results: hide


# Scale frequency count by feature importances
for (i in 1:nrow(importances)) {
  feature <- importances$`Feature Id`[i]
  score <- importances$Weighted_Score[i]
  
  if (feature %in% names(cat_encoded_features)) {
    cat_encoded_features <- cat_encoded_features %>%
      mutate(!!feature := if_else(!!sym(feature) != 0, as.integer(!!sym(feature) * score), !!sym(feature)))

  }
}


head(cat_encoded_features)

```

```{r}

# Distinct Numerical Encodings
numerical_encoding <- cat_encoded_features %>%
  pivot_longer(cols = everything(), names_to = "Feature", values_to = "Encoded_Value") %>% 
  filter(Encoded_Value > 0) %>% 
  unique() %>% 
  arrange(-Encoded_Value)

numerical_encoding %>% 
  datatable()

```


```{r}

# Split Encoded_Feature into Feature and Level using regex to handle backticks and dots
encodings <- numerical_encoding %>%
  mutate(
    Feature_Level = str_split_fixed(Feature, "(?<=`|\\.)", 3),
    Feature = str_remove_all(Feature_Level[, 1], "`|\\."),
    Level = str_remove_all(Feature_Level[, 2], "`|\\.") ,
    Feature = ifelse(Feature == "", str_remove_all(Feature_Level[, 2], "`|\\."), Feature),
    Level = ifelse(Feature_Level[,1] == "`", str_remove_all(Feature_Level[, 3], "`|\\."), Feature_Level[,2])
  ) %>% 
  select(Feature,Level,Encoded_Value) %>% 
  arrange(-Encoded_Value)


encodings %>% 
  datatable()


```


```{r}

# Initialize the updated dataframe
df_updated <- survival_model

# Replace categorical values with numerical encodings
for (feature in unique(encodings$Feature)) {
  encoding_map <- encodings %>%
    filter(Feature == feature) %>%
    select(Level, Encoded_Value) %>%
    deframe()
  
  df_updated[[feature]] <- as.character(df_updated[[feature]])
  df_updated[[feature]] <- as.numeric(encoding_map[df_updated[[feature]]])
}

# Print the updated dataframe to verify
head(df_updated)

```


```{r}

features_cb <- df_updated %>% 
  select(-outcome_final)

# Must be factor or numeric "Class"
model_target <- df_updated %>% 
  select(outcome_final) %>% 
  dummify() %>% 
  as.data.frame()

target_cb <- model_target$outcome_final

```

```{python catboost-r-data}

#| echo: true
#| warning: false
#| message: false
#| results: hide

import numpy as np
from catboost import Pool

# initialize data
X = r.features_cb

y = r.target_cb

# Using this object for feature importance calculations
test_data = catboost_pool = Pool(X, y)


```

#### Hyperparameter Optimization with Optuna

```{python optuna-hyperparameter-optimization, eval=FALSE}

#| eval: false
#| echo: true
#| warning: false
#| message: false
#| results: hide

import numpy as np
from sklearn.model_selection import train_test_split, StratifiedKFold
import pandas as pd
import catboost as cb
import optuna
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score

# Convert target label from list to array
Y = np.array(y)  

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=42)

def objective(trial):
    learning_rate = trial.suggest_float("learning_rate", 0.01, 0.3)
    depth = trial.suggest_int("depth", 3, 9)
    colsample_bylevel = trial.suggest_float("colsample_bylevel", 0.05, 1.0)
    min_data_in_leaf = trial.suggest_int("min_data_in_leaf", 1, 100)
    l2_leaf_reg = trial.suggest_float("l2_leaf_reg", 1, 12)
  
    params = {
        "objective": "Logloss",
        "iterations": 1000,
        "learning_rate": learning_rate,
        "depth": depth,
        "colsample_bylevel": colsample_bylevel,
        "min_data_in_leaf": min_data_in_leaf,
        "boosting_type": "Ordered",
        "bootstrap_type": "MVS",
        "l2_leaf_reg": l2_leaf_reg,
        "early_stopping_rounds": 100,
        "eval_metric": 'Logloss',
    }

    # Initialize CatBoostClassifier with parameters
    model = cb.CatBoostClassifier(**params)

    # Convert training data into a Pool to specify cat features
    train_pool = cb.Pool(X_train, Y_train)

    # Perform cross-validation
    cv_results = cb.cv(train_pool, params, fold_count=3, seed=42, stratified=True, verbose=False)

    # Negative mean Logloss to maximize (since Optuna minimizes)
    mean_logloss = -np.mean(cv_results['test-Logloss-mean'])

    return mean_logloss


study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=100, timeout=6000)

print("Number of finished trials: {}".format(len(study.trials)))
print("Best trial:")
best_trial = study.best_trial

print("  Params: ")
for key, value in best_trial.params.items():
    print("    {}: {}".format(key, value))

```

```{python optuna-params}

model_params3 = {
    'learning_rate': 0.2,
    'depth': 3,
    'colsample_bylevel': 0.06,
    'min_data_in_leaf': 97,
    'l2_leaf_reg': 6.26
}
    
```

```{python catboost-model-logloss}

#| eval: true
#| echo: true
#| message: false
#| warning: false
#| results: hide

from sklearn.model_selection import train_test_split, StratifiedKFold
from catboost import CatBoostClassifier, Pool


X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, train_size=0.80, stratify=y)

model_numeric = CatBoostClassifier(iterations=1000,
                               objective='Logloss',
                               **model_params3, 
                               boosting_type= 'Ordered',
                               bootstrap_type='MVS',
                               early_stopping_rounds=100,
                               use_best_model=True, random_seed=1997)
                               

# Create a Pool object for the training and testing data
train_pool = Pool(X_train, label=Y_train)
test_pool = Pool(X_validation, label=Y_validation)
 

model_numeric.fit(train_pool, eval_set=test_pool)

```
```.text

bestTest = 0.284662381
bestIteration = 58

Shrink model to first 59 iterations.

```
#### Model Accuracy

##### Predictions

```{python}

import pandas as pd

Y_Pred = model_numeric.predict(X_validation)
Y_Pred_Proba = model_numeric.predict_proba(X_validation)[:, 1]  # get the probabilities of the positive class


Y_Pred_Proba_Positive = model_numeric.predict_proba(X_validation)[:, 1]  # Probabilities of the positive class
Y_Pred_Proba_Negative = model_numeric.predict_proba(X_validation)[:, 0]  # Probabilities of the negative class

# Converting predictions and actuals into a DataFrame for better readability, including negative class probabilities
predictions = pd.DataFrame({
    'Prob_Negative_Class': Y_Pred_Proba_Negative,
    'Prob_Positive_Class': Y_Pred_Proba_Positive,
    'Predicted': Y_Pred,
    'Actual': Y_validation
})

# Converting predictions and actuals into a DataFrame for better readability
predictions = pd.DataFrame({
    'Prob_Positive_Class': Y_Pred_Proba,
    'Predicted': Y_Pred,
    'Actual': Y_validation
})

```


```{r message=FALSE, warning=FALSE}

library(probably)

predictions <- py$predictions %>% 
  mutate(Class = ifelse(Actual == 0, "survive", "not_survive"),
         .pred_not_survive = Prob_Positive_Class
         )

# Define the levels you want
factor_levels <- c("survive", "not_survive")

# Set the levels of the 'actuals' column
predictions$Class <- factor(predictions$Class, levels = rev(factor_levels))

predictions %>% 
  datatable()


```

##### Calibration Plot - CatBoost

```{r}

predictions %>% 
  cal_plot_logistic(Class, .pred_not_survive)

```

##### Decision Threshold

```{r message=FALSE, warning=FALSE}

library(pROC)

# Calculate the ROC curve
roc_result <- roc(predictions$Actual, predictions$Prob_Positive_Class)

coords <- coords(roc_result, "best", ret="threshold", best.method="closest.topleft")

# Optimal threshold for maximizing true positive rate
optimal_threshold <- coords$threshold

# Apply the optimal threshold to convert probabilities to class predictions
predictions$predicted_classes <- ifelse(predictions$Prob_Positive_Class >= optimal_threshold, 1, 0)

# Output the optimal threshold
cat("Optimal Threshold:", optimal_threshold, "\n")

cat("Number of '1's predicted:", sum(predictions$predicted_classes), "\n")

```

##### Calibrated Threshold Model Accuracy Using LogLoss

```{python}

#| echo: true
#| warning: false
#| message: false
#| eval: false

from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, roc_curve, roc_auc_score

# Calculate metrics
f1_updated = f1_score(r.predictions["Actual"], r.predictions["predicted_classes"])
precision_updated = precision_score(r.predictions["Actual"], r.predictions["predicted_classes"])
recall_updated = recall_score(r.predictions["Actual"], r.predictions["predicted_classes"])
accuracy_updated = accuracy_score(r.predictions["Actual"], r.predictions["predicted_classes"])

# Calculate the area under the curve (AUC)
model_auc_updated = roc_auc_score(r.predictions["Actual"], r.predictions["Prob_Positive_Class"])
conf_matrix_updated = confusion_matrix(r.predictions["Actual"], r.predictions["predicted_classes"])

```

```{r save-updated-accuracy-metrics}

#| echo: true
#| warning: false
#| message: false
#| eval: false

model_f1_score <- py$f1_updated
model_recall <- py$recall_updated
model_precision <- py$precision_updated
model_accuracy <- py$accuracy_updated
model_roc_score <- py$model_auc_updated

model_accuracy_numeric <- list(
  F1_Score = model_f1_score,
  Recall = model_recall,
  Precision = model_precision,
  Accuracy = model_accuracy,
  ROC_Score = model_roc_score
)

model_accuracy_numeric

```


```{python model-accuracy-calibrated}

#| echo: true
#| results: hide
#| label: fig-model-accuracy-calibrated
#| fig-subcap:
#|   - "Confusion Matrix with Class Imbalance"
#| warning: false
#| message: false
#| eval: false

import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, roc_curve, roc_auc_score
import itertools


# Simplified class names for ticks
class_names = ['Class 0', 'Class 1']
# Detailed class names
detailed_class_names = ['Class 0: Transplanted', 'Class 1: WL Mortality/Deterioration']


normalize = conf_matrix_updated.astype('float') / conf_matrix_updated.sum(axis=1)[:, np.newaxis]

plt.figure(figsize=(10, 7))
plt.imshow(normalize, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix with Class Imbalance')
plt.colorbar()

tick_marks = np.arange(len(class_names))
plt.xticks(tick_marks, class_names, rotation=45)
plt.yticks(tick_marks, class_names)

fmt = '.2f'
for i, j in itertools.product(range(conf_matrix_updated.shape[0]), range(conf_matrix_updated.shape[1])):
    plt.text(j, i, format(conf_matrix_updated[i, j], 'd') + "\n" + format(normalize[i, j], fmt),
             horizontalalignment="center",
             color="white" if normalize[i, j] > 0.5 else "black")

plt.ylabel('True label')
plt.xlabel('Predicted label')

# Show class distribution (imbalance ratio)
class_counts = np.bincount(r.predictions["Actual"])
for i, count in enumerate(class_counts):
    plt.text(i, i, f"\n\nCount: {count}", horizontalalignment="center", verticalalignment="center",
             color="white" if normalize[i, i] > 0.5 else "black")


# Set the titles. Main title and then a subtitle with detailed class names.
plt.suptitle('Confusion Matrix with Class Imbalance', fontsize=16)
plt.title(detailed_class_names, fontsize=10, style='italic')

# Adjust the layout to make room for the main title
plt.tight_layout(rect=[0, 0.03, 1, 0.95]) 

# Save the image in high resolution
plt.savefig('images/model_accuracy_numeric.png', dpi=1200)

```

![One-Hot Encoded Confusion Matrix](images/model_accuracy_numeric.png){#fig-model-accuracy2}


```{python shap-setup}

#| eval: false
#| echo: true
#| message: false
#| warning: false
#| results: hide

import shap

explainer = shap.TreeExplainer(model_numeric)
shap_values = explainer(X)
    
```

### SHAP Values

```{python shap-setup}

#| eval: false
#| echo: true
#| message: false
#| warning: false
#| results: hide

import shap

explainer = shap.TreeExplainer(survival_model)
shap_values = explainer(X)
    
```

##### Beeswarm (Top 14 Features for Risk Calculator)

```{python shap-setup2}

#| eval: false
#| echo: true
#| message: false
#| warning: false
#| results: hide


import matplotlib.pyplot as plt
import shap

shap.initjs()

plt.close()
plt.close()


shap.plots.beeswarm(shap_values,14)
plt.tight_layout()

plt.savefig('images/shapley_beeswarm_risk_calculator.png', dpi=1200)

```

![Beeswarm Chart](images/shapley_beeswarm_chart.png){#fig-shap-beeswarm-chart}

##### Bar Chart for Feature Importance

```{python shapley-bar-chart}

#| eval: false
#| echo: true
#| warning: false
#| message: false
#| results: hide
#| label: fig-net-effect
#| fig-cap: 
#|   - "Net Effect"

import matplotlib.pyplot as plt
import shap

shap.initjs()

plt.close()
plt.close()

shap.plots.bar(shap_values, max_display=14)

plt.tight_layout()

# Save the image in high resolution
plt.savefig('images/shapley_bar_one_hot.png', dpi=1200)

```


![Feature Importance Bar Chart](images/shapley_bar_chart.png){#fig-shap-mean-bar-chart}


##### Shap Summaries R

```{r}

features <- names(features_cb)

# Calculate SHAP values
shap_values <- py$shap_values

# Extract SHAP values for each feature (excluding the last column which is the expected value)
shap_values_matrix <- shap_values$values

# Convert SHAP values to a dataframe for easier analysis
shap_df <- as.data.frame(shap_values_matrix)
names(shap_df) <- shap_values$feature_names

# Print the SHAP values dataframe
print(shap_df)

```

##### Shap Summaries Python

![Kaggle SHAP Value Notebook](https://www.kaggle.com/code/estevaouyra/shap-advanced-uses-subpopulations)

[https://towardsdatascience.com/you-are-underutilizing-shap-values-understanding-populations-and-events-7f4a45202d5](https://towardsdatascience.com/you-are-underutilizing-shap-values-understanding-populations-and-events-7f4a45202d5)

```{python}

def grouped_shap(shap_vals, features, groups):
    groupmap = revert_dict(groups)
    shap_Tdf = pd.DataFrame(shap_vals, columns=pd.Index(features, name='features')).T
    shap_Tdf['group'] = shap_Tdf.reset_index().features.map(groupmap).values
    shap_grouped = shap_Tdf.groupby('group').sum().T
    return shap_grouped

```


### RShiny Model

```{python}

import pandas as pd
import json

sorted_loss = loss.sort_values(by='Importances', ascending=False)

# Get the top 15 rows with the highest importances
top_15_loss = sorted_loss.head(15)

# Combine the lists and remove duplicates
final_top_features = top_15_loss['Feature Id'].tolist()

# Helper function to chunk the list
def chunk_list(lst, n):
    """Yield successive n-sized chunks from lst."""
    for i in range(0, len(lst), n):
        yield lst[i:i + n]

# Chunk the list into sublists of 4 elements each
chunked_packages = list(chunk_list(final_top_features, 4))

# Convert each chunk into a JSON string and collect them into a list
json_chunks = [json.dumps(chunk) for chunk in chunked_packages]

# Join all chunks with a line break to ensure each chunk is on a new line
final_features_top = "\n".join(json_chunks)

print(final_features_top)

```

```{r}

final_features <- py$final_top_features

features_cb <- survival_model_one_hot %>% 
  select(all_of(final_features))

# Must be factor or numeric "Class"
model_target <- survival_model_one_hot %>% 
  select(outcome_final) %>% 
  dummify() %>% 
  as.data.frame()

target_cb <- model_target$outcome_final

```


```{python catboost-r-data}

#| echo: true
#| warning: false
#| message: false
#| results: hide

import numpy as np
from catboost import Pool

# initialize data
X = r.features_cb

y = r.target_cb

# Using this object for feature importance calculations
test_data = catboost_pool = Pool(X, y)

```

#### Hyperparameter Optimization with Optuna

```{python optuna-hyperparameter-optimization, eval=FALSE}

#| eval: false
#| echo: true
#| warning: false
#| message: false
#| results: hide

import numpy as np
from sklearn.model_selection import train_test_split, StratifiedKFold
import pandas as pd
import catboost as cb
import optuna
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score

# Convert target label from list to array
Y = np.array(y)  

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=42)

def objective(trial):
    learning_rate = trial.suggest_float("learning_rate", 0.01, 0.3)
    depth = trial.suggest_int("depth", 3, 9)
    colsample_bylevel = trial.suggest_float("colsample_bylevel", 0.05, 1.0)
    min_data_in_leaf = trial.suggest_int("min_data_in_leaf", 1, 100)
    l2_leaf_reg = trial.suggest_float("l2_leaf_reg", 1, 12)
  
    params = {
        "objective": "Logloss",
        "iterations": 1000,
        "learning_rate": learning_rate,
        "depth": depth,
        "colsample_bylevel": colsample_bylevel,
        "min_data_in_leaf": min_data_in_leaf,
        "boosting_type": "Ordered",
        "bootstrap_type": "MVS",
        "l2_leaf_reg": l2_leaf_reg,
        "early_stopping_rounds": 100,
        "eval_metric": 'Logloss',
    }

    # Initialize CatBoostClassifier with parameters
    model = cb.CatBoostClassifier(**params)

    # Convert training data into a Pool to specify cat features
    train_pool = cb.Pool(X_train, Y_train)

    # Perform cross-validation
    cv_results = cb.cv(train_pool, params, fold_count=3, seed=42, stratified=True, verbose=False)

    # Negative mean Logloss to maximize (since Optuna minimizes)
    mean_logloss = -np.mean(cv_results['test-Logloss-mean'])

    return mean_logloss


study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=100, timeout=6000)

print("Number of finished trials: {}".format(len(study.trials)))
print("Best trial:")
best_trial = study.best_trial

print("  Params: ")
for key, value in best_trial.params.items():
    print("    {}: {}".format(key, value))

```

```{python optuna-params}

model_params = {
    'learning_rate': 0.12,
    'depth': 4,
    'colsample_bylevel': 0.05,
    'min_data_in_leaf': 77,
    'l2_leaf_reg': 4.65
}
    
```

```{python catboost-model-logloss}

#| eval: true
#| echo: true
#| message: false
#| warning: false
#| results: hide

from sklearn.model_selection import train_test_split, StratifiedKFold
from catboost import CatBoostClassifier, Pool

shiny_model = CatBoostClassifier(iterations=1000,
                               objective='Logloss',
                               **model_params, 
                               boosting_type= 'Ordered',
                               bootstrap_type='MVS',
                               early_stopping_rounds=100,
                               use_best_model=True, random_seed=1997)
                               

# Create a Pool object for the training and testing data
train_pool = Pool(X_train, label=Y_train)
test_pool = Pool(X_test, label=Y_test)
 

shiny_model.fit(train_pool, eval_set=test_pool)

gain = shiny_model.get_feature_importance(prettified=True)
loss = shiny_model.get_feature_importance(test_data, type='LossFunctionChange', prettified=True)

```

#### Shiny Model Accuracy

##### Predictions

```{python}

import pandas as pd

Y_Pred = shiny_model.predict(X_test)
Y_Pred_Proba = shiny_model.predict_proba(X_test)[:, 1]  # get the probabilities of the positive class


Y_Pred_Proba_Positive = shiny_model.predict_proba(X_test)[:, 1]  # Probabilities of the positive class
Y_Pred_Proba_Negative = shiny_model.predict_proba(X_test)[:, 0]  # Probabilities of the negative class

# Converting predictions and actuals into a DataFrame for better readability, including negative class probabilities
predictions = pd.DataFrame({
    'Prob_Negative_Class': Y_Pred_Proba_Negative,
    'Prob_Positive_Class': Y_Pred_Proba_Positive,
    'Predicted': Y_Pred,
    'Actual': Y_test
})

# Converting predictions and actuals into a DataFrame for better readability
predictions = pd.DataFrame({
    'Prob_Positive_Class': Y_Pred_Proba,
    'Predicted': Y_Pred,
    'Actual': Y_test
})

```


```{r message=FALSE, warning=FALSE}

library(probably)

predictions <- py$predictions %>% 
  mutate(Class = ifelse(Actual == 0, "survive", "not_survive"),
         .pred_not_survive = Prob_Positive_Class
         )

# Define the levels you want
factor_levels <- c("survive", "not_survive")

# Set the levels of the 'actuals' column
predictions$Class <- factor(predictions$Class, levels = rev(factor_levels))

predictions %>% 
  datatable()


```

##### Calibration Plot - CatBoost

```{r}

predictions %>% 
  cal_plot_logistic(Class, .pred_not_survive)

```

##### Decision Threshold

```{r message=FALSE, warning=FALSE}

library(pROC)

# Calculate the ROC curve
roc_result <- roc(predictions$Actual, predictions$Prob_Positive_Class)

coords <- coords(roc_result, "best", ret="threshold", best.method="closest.topleft")

# Optimal threshold for maximizing true positive rate
optimal_threshold <- coords$threshold

# Apply the optimal threshold to convert probabilities to class predictions
predictions$predicted_classes <- ifelse(predictions$Prob_Positive_Class >= optimal_threshold, 1, 0)

# Output the optimal threshold
cat("Optimal Threshold:", optimal_threshold, "\n")

cat("Number of '1's predicted:", sum(predictions$predicted_classes), "\n")

```

##### Calibrated Threshold Model Accuracy Using LogLoss

```{python}

#| echo: true
#| warning: false
#| message: false
#| eval: false

from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, roc_curve, roc_auc_score

# Calculate metrics
f1_updated = f1_score(r.predictions["Actual"], r.predictions["predicted_classes"])
precision_updated = precision_score(r.predictions["Actual"], r.predictions["predicted_classes"])
recall_updated = recall_score(r.predictions["Actual"], r.predictions["predicted_classes"])
accuracy_updated = accuracy_score(r.predictions["Actual"], r.predictions["predicted_classes"])

# Calculate the area under the curve (AUC)
model_auc_updated = roc_auc_score(r.predictions["Actual"], r.predictions["Prob_Positive_Class"])
conf_matrix_updated = confusion_matrix(r.predictions["Actual"], r.predictions["predicted_classes"])

```

```{r save-updated-accuracy-metrics}

#| echo: true
#| warning: false
#| message: false
#| eval: false

model_f1_score <- py$f1_updated
model_recall <- py$recall_updated
model_precision <- py$precision_updated
model_accuracy <- py$accuracy_updated
model_roc_score <- py$model_auc_updated

model_accuracy_calibrated <- list(
  F1_Score = model_f1_score,
  Recall = model_recall,
  Precision = model_precision,
  Accuracy = model_accuracy,
  ROC_Score = model_roc_score
)

model_accuracy_risk_calculator

```

```{python model-accuracy-calibrated}

#| echo: true
#| results: hide
#| label: fig-model-accuracy-calibrated
#| fig-subcap:
#|   - "Confusion Matrix with Class Imbalance"
#| warning: false
#| message: false
#| eval: false

import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, roc_curve, roc_auc_score
import itertools


# Simplified class names for ticks
class_names = ['Class 0', 'Class 1']
# Detailed class names
detailed_class_names = ['Class 0: Transplanted', 'Class 1: WL Mortality/Deterioration']


normalize = conf_matrix_updated.astype('float') / conf_matrix_updated.sum(axis=1)[:, np.newaxis]

plt.figure(figsize=(10, 7))
plt.imshow(normalize, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix with Class Imbalance')
plt.colorbar()

tick_marks = np.arange(len(class_names))
plt.xticks(tick_marks, class_names, rotation=45)
plt.yticks(tick_marks, class_names)

fmt = '.2f'
for i, j in itertools.product(range(conf_matrix_updated.shape[0]), range(conf_matrix_updated.shape[1])):
    plt.text(j, i, format(conf_matrix_updated[i, j], 'd') + "\n" + format(normalize[i, j], fmt),
             horizontalalignment="center",
             color="white" if normalize[i, j] > 0.5 else "black")

plt.ylabel('True label')
plt.xlabel('Predicted label')

# Show class distribution (imbalance ratio)
class_counts = np.bincount(r.predictions["Actual"])
for i, count in enumerate(class_counts):
    plt.text(i, i, f"\n\nCount: {count}", horizontalalignment="center", verticalalignment="center",
             color="white" if normalize[i, i] > 0.5 else "black")


# Set the titles. Main title and then a subtitle with detailed class names.
plt.suptitle('Confusion Matrix with Class Imbalance', fontsize=16)
plt.title(detailed_class_names, fontsize=10, style='italic')

# Adjust the layout to make room for the main title
plt.tight_layout(rect=[0, 0.03, 1, 0.95]) 

# Save the image in high resolution
plt.savefig('images/model_accuracy_shiny.png', dpi=1200)

```
![Updated Confusion Matrix](images/model_accuracy2.png){#fig-model-accuracy2}


### Regression Model

```{r}

set.seed(1997)

survival_model_one_hot %<>% 
  drop_na()

X <- survival_model_one_hot %>% 
  select(all_of(final_features))

y <- survival_model_one_hot$outcome_final

index <- createDataPartition(y, p = 0.80, list = FALSE, times = 1)
train <- X[index, ]
test <- X[-index, ]
y_train <- y[index]
y_test <- y[-index]


```


```{r lr cache=TRUE}

train_data <- data.frame(y=y_train, train)

glm_model <- glm(y ~ ., data = train_data, family = binomial)

# Summary of the logistic regression model
lr_summary <- summary(glm_model)

# Reorder columns in test_data to match train_data
test <- test[, names(train)]

test_data <- data.frame(y=y_test, test)

# Predict probabilities on test set
lr_proba <- predict(glm_model, newdata = test_data, type = "response")

# Convert probabilities to class predictions
lr_class <- ifelse(lr_proba > 0.5, 1, 0)
names(lr_class) <- "lr_class"

```

```{r}
table(lr_class)
```


### Lasso Penalized Logistic Regression

```{r}

# Create design matrices for both train and test datasets
train_matrix <- model.matrix(~ ., data = train)
test_matrix <- model.matrix(~ ., data = test)

```


```{r lasso-pr cache=TRUE}

library(glmnet)

# Perform cross-validation to select lambda
cv_lasso <- cv.glmnet(train_matrix, y_train, family = "binomial", alpha = 1)

# Predict probabilities on test set
lasso_proba <- predict(cv_lasso, newx = test_matrix, 
                       type = "response", s = "lambda.min")
names(lasso_proba) <- "lasso_proba"

# Convert probabilities to class predictions
lasso_class <- ifelse(lasso_proba > 0.5, 1, 0)
names(lasso_class) <- "lasso_class"

```

```{r}
table(lasso_class)
```

### Lasso Penalized Logistic Regression - Relaxed Fit

```{r lasso-pr-relaxed cache=TRUE, eval=TRUE}

# Perform cross-validation to select lambda
cv_lasso_relaxed <- cv.glmnet(train_matrix, y_train, family = "binomial", alpha = 1, relax = TRUE)

# Predict probabilities on test set
lasso_relaxed_proba <- predict(cv_lasso_relaxed, newx = test_matrix, 
                       type = "response", s = "lambda.min", gamma = "gamma.min")

names(lasso_relaxed_proba) <- "lasso_relaxed_proba"

# Convert probabilities to class predictions
lasso_relaxed_class <- ifelse(lasso_relaxed_proba > 0.5, 1, 0)
names(lasso_relaxed_class) <- "lasso_relaxed_class"

```

```{r}
table(lasso_relaxed_class)
```


